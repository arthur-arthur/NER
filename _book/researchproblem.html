<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Research problem | Multilingual Deep Learning models for Entity Extraction in NLP</title>
  <meta name="description" content="MASTAT thesis" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Research problem | Multilingual Deep Learning models for Entity Extraction in NLP" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="MASTAT thesis" />
  <meta name="github-repo" content="arthur-arthur/MASTAT_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Research problem | Multilingual Deep Learning models for Entity Extraction in NLP" />
  
  <meta name="twitter:description" content="MASTAT thesis" />
  

<meta name="author" content="Arthur Leloup" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multilingual Deep Learning Models for Entity Extraction in NLP</a></li>
<li><a href="./">Arthur Leloup</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="researchproblem.html"><a href="researchproblem.html"><i class="fa fa-check"></i><b>2</b> Research problem</a><ul>
<li class="chapter" data-level="2.1" data-path="researchproblem.html"><a href="researchproblem.html#research-hypotheses"><i class="fa fa-check"></i><b>2.1</b> Research hypotheses</a></li>
<li class="chapter" data-level="2.2" data-path="researchproblem.html"><a href="researchproblem.html#named-entity-recognition"><i class="fa fa-check"></i><b>2.2</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="2.3" data-path="researchproblem.html"><a href="researchproblem.html#static-word-representations"><i class="fa fa-check"></i><b>2.3</b> Static word representations</a></li>
<li class="chapter" data-level="2.4" data-path="researchproblem.html"><a href="researchproblem.html#contextualized-word-representations-and-language-models"><i class="fa fa-check"></i><b>2.4</b> Contextualized word representations and language models</a></li>
<li class="chapter" data-level="2.5" data-path="researchproblem.html"><a href="researchproblem.html#monolingual-versus-multilingual-embeddings"><i class="fa fa-check"></i><b>2.5</b> Monolingual versus multilingual embeddings</a></li>
<li class="chapter" data-level="2.6" data-path="researchproblem.html"><a href="researchproblem.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a><ul>
<li class="chapter" data-level="3.1" data-path="methods.html"><a href="methods.html#data"><i class="fa fa-check"></i><b>3.1</b> Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="methods.html"><a href="methods.html#benchmark-datasets"><i class="fa fa-check"></i><b>3.1.1</b> Benchmark datasets</a></li>
<li class="chapter" data-level="3.1.2" data-path="methods.html"><a href="methods.html#faktion-datasets"><i class="fa fa-check"></i><b>3.1.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="methods.html"><a href="methods.html#word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Word embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="methods.html"><a href="methods.html#static-and-task-specific-embeddings"><i class="fa fa-check"></i><b>3.2.1</b> Static and task-specific embeddings</a></li>
<li class="chapter" data-level="3.2.2" data-path="methods.html"><a href="methods.html#contextualized-word-embeddings"><i class="fa fa-check"></i><b>3.2.2</b> Contextualized word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="methods.html"><a href="methods.html#ner-classifier"><i class="fa fa-check"></i><b>3.3</b> NER classifier</a></li>
<li class="chapter" data-level="3.4" data-path="methods.html"><a href="methods.html#ner-evaluation"><i class="fa fa-check"></i><b>3.4</b> NER evaluation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>4</b> Results</a><ul>
<li class="chapter" data-level="4.1" data-path="results.html"><a href="results.html#benchmark-datasets-1"><i class="fa fa-check"></i><b>4.1</b> Benchmark datasets</a><ul>
<li class="chapter" data-level="4.1.1" data-path="results.html"><a href="results.html#conll2003---english"><i class="fa fa-check"></i><b>4.1.1</b> CoNLL2003 - English</a></li>
<li class="chapter" data-level="4.1.2" data-path="results.html"><a href="results.html#conll2002---dutch"><i class="fa fa-check"></i><b>4.1.2</b> CoNLL2002 - Dutch</a></li>
<li class="chapter" data-level="4.1.3" data-path="results.html"><a href="results.html#wikiner---french"><i class="fa fa-check"></i><b>4.1.3</b> WikiNER - French</a></li>
<li class="chapter" data-level="4.1.4" data-path="results.html"><a href="results.html#trilingual-ner"><i class="fa fa-check"></i><b>4.1.4</b> Trilingual NER</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="results.html"><a href="results.html#faktion-datasets-1"><i class="fa fa-check"></i><b>4.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>5</b> Discussion</a><ul>
<li class="chapter" data-level="5.1" data-path="discussion.html"><a href="discussion.html#benchmarkt-datasets"><i class="fa fa-check"></i><b>5.1</b> Benchmarkt datasets</a></li>
<li class="chapter" data-level="5.2" data-path="discussion.html"><a href="discussion.html#faktion-datasets-2"><i class="fa fa-check"></i><b>5.2</b> Faktion datasets</a></li>
<li class="chapter" data-level="5.3" data-path="discussion.html"><a href="discussion.html#conclusion"><i class="fa fa-check"></i><b>5.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="benchmark-results.html"><a href="benchmark-results.html"><i class="fa fa-check"></i><b>A</b> Benchmark results</a></li>
<li class="chapter" data-level="B" data-path="faktion-results.html"><a href="faktion-results.html"><i class="fa fa-check"></i><b>B</b> Faktion results</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multilingual Deep Learning models for Entity Extraction in NLP</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="researchproblem" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Research problem</h1>
<div id="research-hypotheses" class="section level2">
<h2><span class="header-section-number">2.1</span> Research hypotheses</h2>
<p>The Antwerp-based company <a href="https://www.faktion.com/">Faktion</a> develops machine and deep learning algorithms in the field of computer vision, natural language and sensor data for a wide range of industries. In 2019, they developed <a href="https://metamaze.eu/">Metamaze</a>, a platform to automate complex manual workflows of both structured and unstructured documents. An important part of this platform consists of the automatic annotion of named entities in documents. The NER system consists of 2 consecutive steps: first, the (dominant) language of the document is predicted. Next, a monolingual NER system specific for the predicted language is used to predict entities in the document. This approach has some limitations. The 2-step procedure requires training and maintaining pipelines for each individual language. Also, the monolingual NER system might perform poorly when the system fails to predict the correct language in the first step or when the input document is multilingual.</p>
<p>Using multilingual embeddings might provide a solution to the aforementioned limitations. Even though this would reduce the complexity of the pipeline, the size of the training corpus of a (multilingual) LM is still limited, suggesting that monolingual LMs might internalize more language-specific linguistic knowledge and, hence, provide representations that results in better performance on monolingual NER tasks. In this thesis, we aim to investigate this claim.</p>
<p>We hypothesize that:</p>
<ul>
<li>Monolingual embeddings outperform multilingual embeddings when the input data is monolingual</li>
<li>Multilingual embeddings outperform monolingual embeddings when the input data is multilingual</li>
</ul>
<p>To investigate the first hypothesis, we will use high-quality, well-validated benchmark datasets for NER in English, Dutch and French. In addition, the different NER systems will be evaluated on smaller datasets provided by Faktion that were user-annotated in the Metamaze platform itself. For both the benchmark and Faktion datasets, multilingual dataset will be generated to evaluate the second hypothesis that for the specific (and not very uncommon) case of multilingual input data, multilingual representations outperform monolingual ones in terms of NER performance.</p>
<p>Below, we provide a brief overview of some important concepts related to the aforementioned problem statements. We will briefly introduce the concept of NER and discuss the basic idea of word embeddings, followed by a discussion on language models - probabilistic models that play a pivotal role in enabling computers to acquire linguistic knowledge. Lastly, we discuss how these language models can be used to obtain both monolingual as well as multilingual word embeddings that can be used as input for NER classifiers.</p>
</div>
<div id="named-entity-recognition" class="section level2">
<h2><span class="header-section-number">2.2</span> Named Entity Recognition</h2>
<p>Named Entity Recognition (also known as Entity Extraction) is an NLP-task that aims to automatically locate and annotate pieces of text as <em>entities</em> such as persons (PER), organizations (ORG), geo-political entities (GPE) or locations (LOC), dates etc… (Figure <a href="researchproblem.html#fig:ner-example">2.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:ner-example"></span>
<img src="images/ner_example.png" alt="An example of a simple NER task with the displaCy Named Entity Visualizer." width="100%" />
<p class="caption">
Figure 2.1: An example of a simple NER task with the <a href="https://explosion.ai/demos/displacy-ent">displaCy Named Entity Visualizer</a>.
</p>
</div>

<p>This might appear somewhat trivial at first sight with regular string or pattern matching and a list of known entities, but there are quite some challenges involved. These are for example related to the fact that many words have different meanings (Washington, Apple, Jobs) and that many entities span several words which makes defining their boundaries difficult - even for humans (the New England Journal of Medicine, Swiss Federal Polytechnic School, etc…). NER is essentially a classification problem. The sequential nature of languange renders some algorithms more suited for the task than others. Today, most NER systems are based on neurall networks, as discussed in chapter <a href="methods.html#methods">3</a>. However, no matter which architecture is used, in order to render a NER classifier able to learn how to recognize named entities in text, the input representations must encode the semantic and syntactic properties of words in a meaningful way. Not surprisingly, a lot of the progress in the NER (or any other NLP task) during the past few decades can be attributed to methods that provide these meaningful word representations.</p>
</div>
<div id="static-word-representations" class="section level2">
<h2><span class="header-section-number">2.3</span> Static word representations</h2>
<p>Possibly the simplest solution to impose structure and represent words in a quantitative way would be to use a single one-hot vector <span class="math inline">\(\mathbf{w} \in \mathbb{R}^{|V|}\)</span> for every word in a given vocabulary of size <span class="math inline">\(|V|\)</span>, for example:</p>
<p><span class="math display">\[\begin{align}
\mathbf{w}_{aardvark} = [\ 1\ 0\ 0\ ...\ 0\ 0\ ...\ 0\ 0\ ]^T \notag \\
\mathbf{w}_{cat} = [\ 0\ 0\ 0\ ...\ 1\ 0\ ...\ 0\ 0\ ]^T \notag \\
\mathbf{w}_{zoo} = [\ 0\ 0\ 0\ ...\ 0\ 0\ ...\ 0\ 1\ ]^T \notag 
\end{align}\]</span></p>
<p>This obviously has some limitations: with arguably millions of words in the English language, these vectors are extremely sparse. Secondly, any 2 one-hot vectors are orthogonal and do not encode concepts like semantic similarity:</p>
<p><span class="math display">\[
\mathbf{w}_{cat}^T \mathbf{w}_{dog} = \mathbf{w}_{cat}^T \mathbf{w}_{airplane} = 0
\]</span></p>
<p>It is not unreasonable to assume that there must exist some <span class="math inline">\(n\)</span>-dimensional hyperspace (with <span class="math inline">\(n\)</span> much lower than the size of the entire English vocabulary) that can encode all semantics of a language. Many techniques have been proposed to encode words in a real-numbered space, with the two primary families being matrix factorization methods (such as latent semantic analysis (LSA) <span class="citation">(Deerwester et al. <a href="#ref-deerwester1990">1990</a>)</span>) and iteration based methods based on local context (such as word2vec <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span>). A key concept that many of these different methods have in common is the idea of <em>distributional similarity</em>, an important hypothesis in linguistics (popularized by the British linguist J.R. Firth <span class="citation">(Firth <a href="#ref-firth1957">1957</a>)</span>) that words with a similar meaning occur in a similar context and <em>vice versa</em>.</p>
<p>Matrix factorisation methods typically involve constructing a large matrix - often based on co-occurance counts of words in documents or smaller context windows. Then, dimension-reduction techniques such as truncated singular value decomposition are used to compress the information and obtain low-dimensional vector representations for all words in the vocabulary. Alternatively, instead of performing dimension-reduction on this large matrix, iteration-based methods rely on neural networks that learn iteratively how to represent words in a meaningful way. A major breakthrough - in particular with respect to computational efficiency - was the development of the word2vec framework in 2013 by a team at Google <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span>. The word2vec model is essentially a simplified language model (cf infra) trained to predict whether words are likely to occur together. It appeared that optimizing the model for this task yielded excellent word embeddings that captured a lot of semantic information, with the vector representations of semantically similar words being oriented closely together in typically 300-dimensional embedding space. In addition, the dimensions learned by the model (hence, the relative orientation of the word vectors in the embedding space) even showed to encode meaningful semantic or syntactic concepts, illustrated by low-dimensional linear mappings of some word embeddings as shown in Figure <a href="researchproblem.html#fig:w2v">2.2</a>. Other widely-used word vectors include Stanford’s typically low-dimensional (50-100) GloVe vectors <span class="citation">(Pennington, Socher, and Manning <a href="#ref-pennington2014">2014</a>)</span> that are partly based on word co-occurence count matrices, or Facebook’s 100-300-dimensional fastText vectors <span class="citation">(Bojanowski et al. <a href="#ref-bojanowski2017">2017</a>; Joulin et al. <a href="#ref-joulin2017">2017</a>)</span>.</p>
<p>A problem with these static word embeddings is the way out-of-vocabulary (OOV) words are handled: they don’t provide embeddings for words that were not encountered during training. A possible solution is to learn subword embeddings. The intuition is straightforward: the suffix <em>-shire</em> lets you guess that Melfordshire is probably a location, or the suffix <em>-osis</em> that Myxomatosis might be a sickness<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. Subword-based embeddings have been developed by e.g. fastText through representing words as bags of n-grams <span class="citation">(Bojanowski et al. <a href="#ref-bojanowski2017">2017</a>)</span>. This largely solves the problem of OOV words but results in huge models. An arguably more elegant solution for this problem is provided by BytePair encoding, an unsupervised tokenization algorithm that iteratively merges frequent pairs of characters for a fixed number of iterations <span class="citation">(Sennrich, Haddow, and Birch <a href="#ref-sennrich2016">2016</a>)</span>. Applying this algorithm on a large corpus of text for a well-chosen number of iterations typically yields reasonable subword segementations, where frequent words are represented as such, and rare words as subword tokens. When these tokens are subsequently used to learn embeddings, the resulting BytePair embeddings appear to perform very well when used as input for different NLP tasks.</p>
<div class="figure" style="text-align: center"><span id="fig:w2v"></span>
<img src="images/word_vectors.jpg" alt="Low-dimensional linear mappings of word vectors elegantly demonstrate how the learned dimensions effectively encode meaningful semantic or syntactic concepts like “gender” (left), verb tense (middle) or country-capital relationships (right). Taken from shorturl.at/ivz38." width="100%" />
<p class="caption">
Figure 2.2: Low-dimensional linear mappings of word vectors elegantly demonstrate how the learned dimensions effectively encode meaningful semantic or syntactic concepts like “gender” (left), verb tense (middle) or country-capital relationships (right). Taken from <a href="shorturl.at/ivz38">shorturl.at/ivz38</a>.
</p>
</div>

</div>
<div id="contextualized-word-representations-and-language-models" class="section level2">
<h2><span class="header-section-number">2.4</span> Contextualized word representations and language models</h2>
<p>Even though word embeddings like word2vec and fastText led to significant improvements in the NLP field, they suffer from a major limitation: many words have different meanings and these static word embeddings aggregate all these different meanings into a single vector <span class="citation">(Arora et al. <a href="#ref-arora2018">2018</a>)</span>. This is especially problematic for polysemous words like “play”, “Washington” or “mouse”. These issues have been addressed in the recent years with the development of so-called contextualized word embeddings - i.e. embeddings that represent words in their context, meaning that a single polysemous word like “Washington” has a different word embedding, depending on the context it appears in. An alternative approach would be to obtain representations by including randomly-initialized vectors as parameters of a NER classifier or to train task-specific character-feature embeddings on the NER training data directly, as described in <span class="citation">(Lample et al. <a href="#ref-lample2016">2016</a>)</span>. We will refer to these task-specific representations as one hot word type embeddings (OHEs) and character embeddings, respectively, and discuss them further in chapter <a href="methods.html#methods">3</a>. However, learning these task-specific representations from scratch for every NER task is not very efficient and a much more successful approach is to use contextualized word representations obtained from pretrained neural language models (LMs), as discussed below.</p>
<p>Neural LMs are probabilistic classifiers that are trained to predict the probability of a word given the previous words. The concept of LMs has played a crucial role in NLP for many decades (long before the rise of deep learning in NLP) as they allow to predict words given some context as input or provide joint probabilities of word sequences (Equation <a href="researchproblem.html#eq:chainrule">(2.1)</a>). A good LM will thus assign a higher joint probability to the sequence “I like bikes” as compared to “Airplane cat want is” because the former sequence is semantically and syntactically valid, while the latter sequence makes no sense. This renders them very useful for many NLP tasks. Yet, the underlying idea of the LM objective is surprisingly simple. The joint probability of a sequence of <span class="math inline">\(m\)</span> words (<span class="math inline">\({w_1, w_2, ..., w_m}\)</span>) can be decomposed into a product of conditional probabilities <span class="citation">(Jurafsky and Martin <a href="#ref-jurafsky2019">2019</a>)</span>:</p>
<p><span class="math display" id="eq:chainrule">\[\begin{equation}
P\left(w_1, ..., w_m\right) = \prod_{i=1}^m P\left(w_i\mid w_1, ..., w_{i-1}\right)
\tag{2.1}
\end{equation}\]</span></p>
<p>A so-called forward LM is trained to estimate the conditional probability of a word <span class="math inline">\(w_i\)</span> given the previous words <span class="math inline">\({w_1,..., w_{i-1}}\)</span>. Since the number of all possible word sequences for a vocabulary of size <span class="math inline">\(|V|\)</span> is <span class="math inline">\(|V|^m\)</span>, estimating these conditional probabilities by simply counting how often every sequence (<span class="math inline">\(w_1,..., w_{i-1}\)</span>) is followed by <span class="math inline">\(w_i\)</span> in a corpus is practically impossible (i.e. many valid sentences will never occur in a reasonably-sized training corpus). Traditionally, this was often solved by approximating the entire history of a word by a fixed window of size <span class="math inline">\(n - 1\)</span> (with n typically not larger than 5):</p>
<p><span class="math display">\[\begin{equation}
P\left(w_1, ..., w_m\right) \approx \prod_{i=1}^mP\left(w_i \mid w_{i - n + 1}, ..., w_{i-1}\right)
\end{equation}\]</span></p>
<p>This so-called Markov assumption <span class="citation">(Manning <a href="#ref-manning2019">2019</a><a href="#ref-manning2019">b</a>)</span> reduces the number of all possible sequences from <span class="math inline">\(|V|^m\)</span> to <span class="math inline">\(|V|^n\)</span>, making it practically feasible to estimate these conditional probabilities by obtaining relative counts of all <span class="math inline">\(n\)</span>-grams in a corpus (a simple application of Bayes’ rule). Even though it is clear that the Markov assumption is not strictly valid for language, these so-called <span class="math inline">\(n\)</span>-gram models work surprisingly well, are very efficient to train and, hence, still used for some applications today.</p>
<p>However, for many NLP tasks, having access to information that might be arbitrarily far away in the sequence is required to perform well, and <span class="math inline">\(n\)</span>-gram models are intrinsically limited in terms of the context they can consider. Luckily, more complex model architectures like Recurrent Neural Networks (RNNs), Long Short-term Memory (LSTM) or Transformers are able to handle much longer histories, rendering them much better at the LM task. These architectures will be discussed in chapter <a href="methods.html#methods">3</a>. There has been a steady rise in the quality and applications of (deep) neural LMs over the past 1-2 decades, with modern neural LMs being able to internalize an incredible amount of linguistic knowledge from huge corpora of text. While there are major differences in terms of the neural architectures and training algorithms, a key concept that drives a lot of these models’ ability to internalize linguistic knowledge is the LM task: predicting a word given some context as input. When trained well, these models are able to provide probability distributions over word sequences or predict words when given some context, rendering them extremely useful in applications such as auto-completion tasks, text generation, machine-translation or speech recognition. In addition, the contextual information (that may extend back to the very beginning of the sequence) that is considered by the model to predict the next word in a sequence is essentially encoded in the hidden states of these neural LMs. Recent attempts to use these representations as contextualized word embeddings for different NLP tasks - including NER - have been very successful, as discussed in chapter <a href="methods.html#methods">3</a>.</p>
</div>
<div id="monolingual-versus-multilingual-embeddings" class="section level2">
<h2><span class="header-section-number">2.5</span> Monolingual versus multilingual embeddings</h2>
<p>Today, pretrained static word embeddings like word2vec <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span>, fastText <span class="citation">(Bojanowski et al. <a href="#ref-bojanowski2017">2017</a>)</span> and GloVe <span class="citation">(Pennington, Socher, and Manning <a href="#ref-pennington2014">2014</a>)</span> are available for a wide range of languages. Similarly, many of the previously discussed neural LMs provide contextualized word representations. The recently developed character-based Flair LM <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span> or Transformer-based BERT model <span class="citation">(Devlin et al. <a href="#ref-devlin2019">2019</a>)</span> (both models are discussed in chapter <a href="methods.html#methods">3</a>) have been pre-trained in several languages and are often made freely available. Examples include CamemBERT <span class="citation">(Martin et al. <a href="#ref-martin2019">2019</a>)</span> and BERTje <span class="citation">(de Vries et al. <a href="#ref-devries2019">2019</a>)</span>, a French and Dutch version of BERT, respectively, or the Dutch and French versions of Flair made available in the <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md">Flair’s NLP library</a>.</p>
<p>These previously-discussed frameworks provide word representations in a monolingual semantic space. As mentioned previously, this has some limitations that are directly related to the research problems formulated earlier. For example, implementing an existing NLP task for a similar problem in a different language involves starting from scratch. Training and maintaining multiple pipelines for different languages has to be done independently. Many words - especially named entities - are similar across languages, suggesting that having different independent monolingual embeddings to represent these entities in different languages might not be very efficient. Additional problems with monolingual embeddings arise when the input data is multilingual.</p>
<p>These problems have been partly addressed through unsupervised or semi-supervised algorithms that jointly map monolingual word embeddings into a common multilingual embedding space such that words with a similar meaning are aligned <span class="citation">(Artetxe, Labaka, and Agirre <a href="#ref-artetxe2018">2018</a>; Lample et al. <a href="#ref-lample2018">2018</a>)</span>. In addition, the aforementioned BytePair encoding algorithm has been used to create (static) multilingual BytePair embeddings based on a single corpus of Wikipedia articles in 275 languages <span class="citation">(Heinzerling and Strube <a href="#ref-heinzerling2017">2017</a>)</span>. With the efficiency of BERT’s transformer architecture and Flair’s character-based LM objective, the question then arises whether these models can be jointly trained over a large corpus of text in many different languages to learn contextualized encodings for a multilingual vocabulary in a single embedding space without having to explicitely align different monolingual embeddings. This has been attempted for both the BERT and Flair models: multilingual BERT (mBERT)<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> was trained on a corpora consisting of the 100 most common languages on Wikipedia and the multilingual version of Flair (mFlair) was trained on over 300 languages<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Since a single multilingual vocabulary was provided as training data, these models are hypothesized to learn word meaning in a language-independent way such that the vector representations for e.g. <em>vélo</em>, <em>bike</em> and <em>fiets</em> are located close to each other in the embedding space. Potential applications for such language-independent semantic representations are for example so-called zero-shot cross-lingual model transfer. This involves training a downstream task like NER using data from a high-resource language like English and effectively applying the learned classifier in another language like Dutch. Experiments with mBERT have shown that this approach works surprisingly well, indicating that mBERT embeddings for words in different languages share a significant subspace that encodes semantic information in a language-independent way <span class="citation">(Pires, Schlinger, and Garrette <a href="#ref-pires2019">2019</a>)</span>.</p>
</div>
<div id="summary" class="section level2">
<h2><span class="header-section-number">2.6</span> Summary</h2>
<p>To summarize, in this thesis, we aim to evaluate how state-of-the-art monolingual and monolingual embeddings affect the performance of different NER tasks. To challenge the research hypotheses mentioned at the beginning of this chapter, we will use both monolingual and multilingual datasets to train and evaluate different NER classifiers using different embedding types. In order to acquire a better understanding on how different embedding types affect NER performance, we will not only evaluate single monolingual and multilingual embeddings, but also create so-called stacked embeddings by concatenating different word representations into a single, high-dimensional vector. In the next chapter, we introduce the different embedding types that were used for the experiments presented in this thesis.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-akbik2018">
<p>Akbik, Alan, Duncan Blythe, and Roland Vollgraf. 2018. “Contextual String Embeddings for Sequence Labeling.” In <em>Proceedings of the 27th International Conference on Computational Linguistics</em>, 1638–49. Santa Fe, New Mexico, USA: Association for Computational Linguistics.</p>
</div>
<div id="ref-arora2018">
<p>Arora, Sanjeev, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2018. “Linear Algebraic Structure of Word Senses, with Applications to Polysemy.” <em>Transactions of the Association for Computational Linguistics</em> 6: 483–95. <a href="https://doi.org/10.1162/tacl_a_00034">https://doi.org/10.1162/tacl_a_00034</a>.</p>
</div>
<div id="ref-artetxe2018">
<p>Artetxe, Mikel, Gorka Labaka, and Eneko Agirre. 2018. “A Robust Self-Learning Method for Fully Unsupervised Cross-Lingual Mappings of Word Embeddings.” In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 789–98. Melbourne, Australia: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P18-1073">https://doi.org/10.18653/v1/P18-1073</a>.</p>
</div>
<div id="ref-bojanowski2017">
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. “Enriching Word Vectors with Subword Information.” <em>Transactions of the Association for Computational Linguistics</em> 5. MIT Press: 135–46.</p>
</div>
<div id="ref-deerwester1990">
<p>Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. “Indexing by Latent Semantic Analysis.” <em>Journal of the American Society for Information Science</em> 41 (6): 391–407. <a href="https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9">https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9</a>.</p>
</div>
<div id="ref-devlin2019">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div id="ref-devries2019">
<p>de Vries, Wietse, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. “BERTje: A Dutch BERT Model.” <em>arXiv Preprint arXiv:1912.09582</em>.</p>
</div>
<div id="ref-firth1957">
<p>Firth, John Rupert. 1957. <em>A Synopsis of Linguistic Theory 1930-1955</em>. Philological Society, Oxford.</p>
</div>
<div id="ref-heinzerling2017">
<p>Heinzerling, Benjamin, and Michael Strube. 2017. “BPEmb: Tokenization-Free Pre-Trained Subword Embeddings in 275 Languages.” <em>arXiv:1710.02187 [Cs]</em>, October. <a href="http://arxiv.org/abs/1710.02187">http://arxiv.org/abs/1710.02187</a>.</p>
</div>
<div id="ref-joulin2017">
<p>Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. “Bag of Tricks for Efficient Text Classification.” In <em>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, 427–31. Valencia, Spain: Association for Computational Linguistics.</p>
</div>
<div id="ref-jurafsky2019">
<p>Jurafsky, Daniel, and James H. Martin. 2019. <em>Speech and Language Processing</em>. 3th ed. draft.</p>
</div>
<div id="ref-lample2016">
<p>Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. “Neural Architectures for Named Entity Recognition.” In <em>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 260–70. San Diego, California: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N16-1030">https://doi.org/10.18653/v1/N16-1030</a>.</p>
</div>
<div id="ref-lample2018">
<p>Lample, Guillaume, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herv’e J’egou. 2018. “Word Translation Without Parallel Data.”</p>
</div>
<div id="ref-manning2019">
<p>Manning, Christopher. 2019b. “Stanford CS 224N | Natural Language Processing with Deep Learning.” http://web.stanford.edu/class/cs224n/.</p>
</div>
<div id="ref-martin2019">
<p>Martin, Louis, Benjamin Muller, Pedro Javier Ortiz Su’arez, Yoann Dupont, Laurent Romary, ’Eric Villemonte de la Clergerie, Djam’e Seddah, and Benoît Sagot. 2019. “CamemBERT: A Tasty French Language Model.” <em>arXiv Preprint arXiv:1911.03894</em>.</p>
</div>
<div id="ref-mikolov2013">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv:1301.3781 [Cs]</em>, September. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>
</div>
<div id="ref-pennington2014">
<p>Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “Glove: Global Vectors for Word Representation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.</p>
</div>
<div id="ref-pires2019">
<p>Pires, Telmo, Eva Schlinger, and Dan Garrette. 2019. “How Multilingual Is Multilingual BERT?” <em>arXiv Preprint arXiv:1906.01502</em>.</p>
</div>
<div id="ref-sennrich2016">
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1715–25. Berlin, Germany: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P16-1162">https://doi.org/10.18653/v1/P16-1162</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p><a href="https://nlp.h-its.org/bpemb/#">Example taken from: https://nlp.h-its.org/bpemb/</a><a href="researchproblem.html#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p><a href="https://github.com/google-research/bert/blob/master/multilingual.md">GitHub: google-research/bert/multilingual</a><a href="researchproblem.html#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p><a href="https://github.com/stefan-it/flair-pos-tagging">Github: stefan-it/flair-pos-tagging</a><a href="researchproblem.html#fnref4" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/arthur-arthur/MASTAT_thesis/edit/master/02-research-problem.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Thesis_Arthur_Leloup.pdf", "Thesis_Arthur_Leloup.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
