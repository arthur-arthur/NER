<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Multilingual Deep Learning models for Entity Extraction in NLP</title>
  <meta name="description" content="MASTAT thesis" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Multilingual Deep Learning models for Entity Extraction in NLP" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="MASTAT thesis" />
  <meta name="github-repo" content="arthur-arthur/MASTAT_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Multilingual Deep Learning models for Entity Extraction in NLP" />
  
  <meta name="twitter:description" content="MASTAT thesis" />
  

<meta name="author" content="Arthur Leloup" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="researchproblem.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multilingual Deep Learning Models for Entity Extraction in NLP</a></li>
<li><a href="./">Arthur Leloup</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="researchproblem.html"><a href="researchproblem.html"><i class="fa fa-check"></i><b>2</b> Research problem</a><ul>
<li class="chapter" data-level="2.1" data-path="researchproblem.html"><a href="researchproblem.html#research-hypotheses"><i class="fa fa-check"></i><b>2.1</b> Research hypotheses</a></li>
<li class="chapter" data-level="2.2" data-path="researchproblem.html"><a href="researchproblem.html#named-entity-recognition"><i class="fa fa-check"></i><b>2.2</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="2.3" data-path="researchproblem.html"><a href="researchproblem.html#static-word-representations"><i class="fa fa-check"></i><b>2.3</b> Static word representations</a></li>
<li class="chapter" data-level="2.4" data-path="researchproblem.html"><a href="researchproblem.html#contextualized-word-representations-and-language-models"><i class="fa fa-check"></i><b>2.4</b> Contextualized word representations and language models</a></li>
<li class="chapter" data-level="2.5" data-path="researchproblem.html"><a href="researchproblem.html#monolingual-versus-multilingual-embeddings"><i class="fa fa-check"></i><b>2.5</b> Monolingual versus multilingual embeddings</a></li>
<li class="chapter" data-level="2.6" data-path="researchproblem.html"><a href="researchproblem.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a><ul>
<li class="chapter" data-level="3.1" data-path="methods.html"><a href="methods.html#data"><i class="fa fa-check"></i><b>3.1</b> Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="methods.html"><a href="methods.html#benchmark-datasets"><i class="fa fa-check"></i><b>3.1.1</b> Benchmark datasets</a></li>
<li class="chapter" data-level="3.1.2" data-path="methods.html"><a href="methods.html#faktion-datasets"><i class="fa fa-check"></i><b>3.1.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="methods.html"><a href="methods.html#word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Word embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="methods.html"><a href="methods.html#static-and-task-specific-embeddings"><i class="fa fa-check"></i><b>3.2.1</b> Static and task-specific embeddings</a></li>
<li class="chapter" data-level="3.2.2" data-path="methods.html"><a href="methods.html#contextualized-word-embeddings"><i class="fa fa-check"></i><b>3.2.2</b> Contextualized word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="methods.html"><a href="methods.html#ner-classifier"><i class="fa fa-check"></i><b>3.3</b> NER classifier</a></li>
<li class="chapter" data-level="3.4" data-path="methods.html"><a href="methods.html#ner-evaluation"><i class="fa fa-check"></i><b>3.4</b> NER evaluation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>4</b> Results</a><ul>
<li class="chapter" data-level="4.1" data-path="results.html"><a href="results.html#benchmark-datasets-1"><i class="fa fa-check"></i><b>4.1</b> Benchmark datasets</a><ul>
<li class="chapter" data-level="4.1.1" data-path="results.html"><a href="results.html#conll2003---english"><i class="fa fa-check"></i><b>4.1.1</b> CoNLL2003 - English</a></li>
<li class="chapter" data-level="4.1.2" data-path="results.html"><a href="results.html#conll2002---dutch"><i class="fa fa-check"></i><b>4.1.2</b> CoNLL2002 - Dutch</a></li>
<li class="chapter" data-level="4.1.3" data-path="results.html"><a href="results.html#wikiner---french"><i class="fa fa-check"></i><b>4.1.3</b> WikiNER - French</a></li>
<li class="chapter" data-level="4.1.4" data-path="results.html"><a href="results.html#trilingual-ner"><i class="fa fa-check"></i><b>4.1.4</b> Trilingual NER</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="results.html"><a href="results.html#faktion-datasets-1"><i class="fa fa-check"></i><b>4.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>5</b> Discussion</a><ul>
<li class="chapter" data-level="5.1" data-path="discussion.html"><a href="discussion.html#benchmarkt-datasets"><i class="fa fa-check"></i><b>5.1</b> Benchmarkt datasets</a></li>
<li class="chapter" data-level="5.2" data-path="discussion.html"><a href="discussion.html#faktion-datasets-2"><i class="fa fa-check"></i><b>5.2</b> Faktion datasets</a></li>
<li class="chapter" data-level="5.3" data-path="discussion.html"><a href="discussion.html#conclusion"><i class="fa fa-check"></i><b>5.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="benchmark-results.html"><a href="benchmark-results.html"><i class="fa fa-check"></i><b>A</b> Benchmark results</a></li>
<li class="chapter" data-level="B" data-path="faktion-results.html"><a href="faktion-results.html"><i class="fa fa-check"></i><b>B</b> Faktion results</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multilingual Deep Learning models for Entity Extraction in NLP</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<p>Natural Language Processing (NLP) is a subfield of linguistics, computer science and statistics that aims to design algorithms that allow interaction and understanding between computers and human languages. Human languages are highly complex and the understanding of language typically requires a huge amount of prior knowledge that we gradually acquire during our lives. We are able to infer a massive amount of information from language, often only from subtle pieces of non-linguistic information, such as the context it is used in.</p>
<p>In this thesis, we focus on Named Entity Recognition (NER), a NLP-task that consists of identifying names of persons, locations, organizations or other so-called “named entities” in text. Since text is a form of unstructured data, a key challenge in NER (and NLP in general) is to find strategies that effectively impose structure and provide numeric representations of the semantic and syntactic properties of each word in the input text in such a way that it renders a statistical NER model able to predict the entity label associated with each word in the input text.</p>
<p>Around 2013, static word embeddings like word2vec <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span> became a standard component of most NER systems due to their ability to capture semantic information in a dense, numeric vector. They are, however, limited in the fact that they aggregate all different meanings of a word into a single vector. This issue was addressed around 5 years later, when context-dependent word representations obtained from large, pretrained (deep) neural language models (LMs) began to outperform static word embeddings. Neural LMs are models that are trained on the LM objective, i.e. predicting the conditional distribution of a word in context, in a completely unsupervised way. This implicitely forces the model to internalize semantic and syntactic concepts. As such, the activations learned by these models provide very meaningful and context-dependent word representations that have shown to be very useful to represent words in typically smaller, supervised NLP tasks like NER. Since the release of ImageNet <span class="citation">(Deng et al. <a href="#ref-deng2009">2009</a>)</span> in 2009, this concept of transfering knowledge from large, pretrained models to improve the performance of smaller - typically supervised - tasks has been applied succesfully in the field of computer vision. With the rise of the aforementioned pretrained (deep) neural LMs that provide excellent contextualized word embeddings, this concept of “transfer learning” has also been driving most of the progress in the NLP field during the past 3 years.</p>
<p>Recently, the improved training efficiency of character-based LMs like Flair <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span> or Transformer-based BERT <span class="citation">(Devlin et al. <a href="#ref-devlin2019">2019</a>)</span> have even allowed to feed large corpora of text in many different languages to train multilingual LMs. The internal representations of these multilingual LMs are hypothesized to provide contextualized, language-independent semantic word representations <span class="citation">(Pires, Schlinger, and Garrette <a href="#ref-pires2019">2019</a>)</span>. There are many potential applications where these multilingual embeddings could address many of the limitations of monolingual embeddings, including in NER. However, the difference of multilingual embeddings and monolingual embeddings in terms of downstream NER performance are not well-described.</p>
<p>The main goal of this thesis is to compare how the use of monolingual or multilingual contextualized word embeddings affects performance of a specific Named Entity Recognition (NER) task when applied on monolingual (English, Dutch and French) and multilingual data. We used a Bidirectional Long Short-Term Memory NER tagger with a Conditional Random Field decoding layer (BiLSTM-CRF), a neural network architecture that has shown to provide state-of-the-art performance on many NER tasks<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Monolingual and multilingual pretrained LMs were used to obtain contextualized monolingual and multilingual word representations. Since it has been shown that NER performance might benefit from concatenating these contextualized representations with task-specific <span class="citation">(Lample et al. <a href="#ref-lample2016">2016</a>)</span> and/or static word vectors like word2vec <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span> or GloVe <span class="citation">(Pennington, Socher, and Manning <a href="#ref-pennington2014">2014</a>)</span>, different so-called stacked embeddings will be evaluated as well.</p>
<p>We first reproduced benchmarks for English, Dutch and French NER using the widely-used human-annotated datasets CoNLL <span class="citation">(Tjong Kim Sang <a href="#ref-tjongkimsang2002">2002</a>; Tjong Kim Sang and De Meulder <a href="#ref-tjongkimsang2003">2003</a>)</span> (for Dutch and English) and WikiNER for French <span class="citation">(Nothman et al. <a href="#ref-nothman2013">2013</a>)</span>. To evaluate how NER performance is affected by multilingual and monolingual embeddings when the input data itself is multilingual, we constructed multilingual datasets by merging the aforementioned monolingual datasets. Next, the pipelines were trained and evaluated on “real-world” Dutch and French monolingual datasets as well as a multilingual dataset, provided by the company Faktion.</p>
<p>In chapter <a href="researchproblem.html#researchproblem">2</a>, we provide a detailed overview of the research questions and introduce some important theoretical concepts related to monolingual and multilingual word embeddings in the context of NER. In chapter <a href="methods.html#methods">3</a> we introduce the datasets used to evaluate our NER systems, provide a brief overview of some common neural LM architectures and how they have contributed to the state-of-the-art performance of many NER systems that are used today. We also introduce how the BiLSTM-CRF architecture was trained to perform NER and how we evaluated the performance of the different NER systems. In chapter <a href="results.html#results">4</a>, we report the main results. Lastly, in chapter <a href="discussion.html#discussion">5</a>, we summarize the main conclusions, discuss some of the limitations of this study and provide some future perspectives.</p>

</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-akbik2018">
<p>Akbik, Alan, Duncan Blythe, and Roland Vollgraf. 2018. “Contextual String Embeddings for Sequence Labeling.” In <em>Proceedings of the 27th International Conference on Computational Linguistics</em>, 1638–49. Santa Fe, New Mexico, USA: Association for Computational Linguistics.</p>
</div>
<div id="ref-deng2009">
<p>Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.</p>
</div>
<div id="ref-devlin2019">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div id="ref-lample2016">
<p>Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. “Neural Architectures for Named Entity Recognition.” In <em>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 260–70. San Diego, California: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N16-1030">https://doi.org/10.18653/v1/N16-1030</a>.</p>
</div>
<div id="ref-mikolov2013">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv:1301.3781 [Cs]</em>, September. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>
</div>
<div id="ref-nothman2013">
<p>Nothman, Joel, Nicky Ringland, Will Radford, Tara Murphy, and James R. Curran. 2013. “Learning Multilingual Named Entity Recognition from Wikipedia.” <em>Artificial Intelligence</em>, Artificial Intelligence, Wikipedia and Semi-Structured Resources, 194 (January): 151–75. <a href="https://doi.org/10.1016/j.artint.2012.03.006">https://doi.org/10.1016/j.artint.2012.03.006</a>.</p>
</div>
<div id="ref-pennington2014">
<p>Pennington, Jeffrey, Richard Socher, and Christopher Manning. 2014. “Glove: Global Vectors for Word Representation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43. Doha, Qatar: Association for Computational Linguistics. <a href="https://doi.org/10.3115/v1/D14-1162">https://doi.org/10.3115/v1/D14-1162</a>.</p>
</div>
<div id="ref-pires2019">
<p>Pires, Telmo, Eva Schlinger, and Dan Garrette. 2019. “How Multilingual Is Multilingual BERT?” <em>arXiv Preprint arXiv:1906.01502</em>.</p>
</div>
<div id="ref-tjongkimsang2002">
<p>Tjong Kim Sang, Erik F. 2002. “Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.” In <em>COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002)</em>.</p>
</div>
<div id="ref-tjongkimsang2003">
<p>Tjong Kim Sang, Erik F., and Fien De Meulder. 2003. “Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.” In <em>Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</em>, 142–47.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://nlpprogress.com/english/named_entity_recognition.html">http://nlpprogress.com/english/named_entity_recognition.html</a><a href="introduction.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="researchproblem.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/arthur-arthur/MASTAT_thesis/edit/master/01-introduction.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Thesis_Arthur_Leloup.pdf", "Thesis_Arthur_Leloup.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
