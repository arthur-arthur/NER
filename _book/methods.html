<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Methods | Multilingual Deep Learning models for Entity Extraction in NLP</title>
  <meta name="description" content="MASTAT thesis" />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Methods | Multilingual Deep Learning models for Entity Extraction in NLP" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="MASTAT thesis" />
  <meta name="github-repo" content="arthur-arthur/MASTAT_thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Methods | Multilingual Deep Learning models for Entity Extraction in NLP" />
  
  <meta name="twitter:description" content="MASTAT thesis" />
  

<meta name="author" content="Arthur Leloup" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="researchproblem.html"/>
<link rel="next" href="results.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multilingual Deep Learning Models for Entity Extraction in NLP</a></li>
<li><a href="./">Arthur Leloup</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="researchproblem.html"><a href="researchproblem.html"><i class="fa fa-check"></i><b>2</b> Research problem</a><ul>
<li class="chapter" data-level="2.1" data-path="researchproblem.html"><a href="researchproblem.html#research-hypotheses"><i class="fa fa-check"></i><b>2.1</b> Research hypotheses</a></li>
<li class="chapter" data-level="2.2" data-path="researchproblem.html"><a href="researchproblem.html#named-entity-recognition"><i class="fa fa-check"></i><b>2.2</b> Named Entity Recognition</a></li>
<li class="chapter" data-level="2.3" data-path="researchproblem.html"><a href="researchproblem.html#static-word-representations"><i class="fa fa-check"></i><b>2.3</b> Static word representations</a></li>
<li class="chapter" data-level="2.4" data-path="researchproblem.html"><a href="researchproblem.html#contextualized-word-representations-and-language-models"><i class="fa fa-check"></i><b>2.4</b> Contextualized word representations and language models</a></li>
<li class="chapter" data-level="2.5" data-path="researchproblem.html"><a href="researchproblem.html#monolingual-versus-multilingual-embeddings"><i class="fa fa-check"></i><b>2.5</b> Monolingual versus multilingual embeddings</a></li>
<li class="chapter" data-level="2.6" data-path="researchproblem.html"><a href="researchproblem.html#summary"><i class="fa fa-check"></i><b>2.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>3</b> Methods</a><ul>
<li class="chapter" data-level="3.1" data-path="methods.html"><a href="methods.html#data"><i class="fa fa-check"></i><b>3.1</b> Data</a><ul>
<li class="chapter" data-level="3.1.1" data-path="methods.html"><a href="methods.html#benchmark-datasets"><i class="fa fa-check"></i><b>3.1.1</b> Benchmark datasets</a></li>
<li class="chapter" data-level="3.1.2" data-path="methods.html"><a href="methods.html#faktion-datasets"><i class="fa fa-check"></i><b>3.1.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="methods.html"><a href="methods.html#word-embeddings"><i class="fa fa-check"></i><b>3.2</b> Word embeddings</a><ul>
<li class="chapter" data-level="3.2.1" data-path="methods.html"><a href="methods.html#static-and-task-specific-embeddings"><i class="fa fa-check"></i><b>3.2.1</b> Static and task-specific embeddings</a></li>
<li class="chapter" data-level="3.2.2" data-path="methods.html"><a href="methods.html#contextualized-word-embeddings"><i class="fa fa-check"></i><b>3.2.2</b> Contextualized word embeddings</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="methods.html"><a href="methods.html#ner-classifier"><i class="fa fa-check"></i><b>3.3</b> NER classifier</a></li>
<li class="chapter" data-level="3.4" data-path="methods.html"><a href="methods.html#ner-evaluation"><i class="fa fa-check"></i><b>3.4</b> NER evaluation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>4</b> Results</a><ul>
<li class="chapter" data-level="4.1" data-path="results.html"><a href="results.html#benchmark-datasets-1"><i class="fa fa-check"></i><b>4.1</b> Benchmark datasets</a><ul>
<li class="chapter" data-level="4.1.1" data-path="results.html"><a href="results.html#conll2003---english"><i class="fa fa-check"></i><b>4.1.1</b> CoNLL2003 - English</a></li>
<li class="chapter" data-level="4.1.2" data-path="results.html"><a href="results.html#conll2002---dutch"><i class="fa fa-check"></i><b>4.1.2</b> CoNLL2002 - Dutch</a></li>
<li class="chapter" data-level="4.1.3" data-path="results.html"><a href="results.html#wikiner---french"><i class="fa fa-check"></i><b>4.1.3</b> WikiNER - French</a></li>
<li class="chapter" data-level="4.1.4" data-path="results.html"><a href="results.html#trilingual-ner"><i class="fa fa-check"></i><b>4.1.4</b> Trilingual NER</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="results.html"><a href="results.html#faktion-datasets-1"><i class="fa fa-check"></i><b>4.2</b> Faktion datasets</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="discussion.html"><a href="discussion.html"><i class="fa fa-check"></i><b>5</b> Discussion</a><ul>
<li class="chapter" data-level="5.1" data-path="discussion.html"><a href="discussion.html#benchmarkt-datasets"><i class="fa fa-check"></i><b>5.1</b> Benchmarkt datasets</a></li>
<li class="chapter" data-level="5.2" data-path="discussion.html"><a href="discussion.html#faktion-datasets-2"><i class="fa fa-check"></i><b>5.2</b> Faktion datasets</a></li>
<li class="chapter" data-level="5.3" data-path="discussion.html"><a href="discussion.html#conclusion"><i class="fa fa-check"></i><b>5.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="benchmark-results.html"><a href="benchmark-results.html"><i class="fa fa-check"></i><b>A</b> Benchmark results</a></li>
<li class="chapter" data-level="B" data-path="faktion-results.html"><a href="faktion-results.html"><i class="fa fa-check"></i><b>B</b> Faktion results</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multilingual Deep Learning models for Entity Extraction in NLP</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="methods" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Methods</h1>
<div id="data" class="section level2">
<h2><span class="header-section-number">3.1</span> Data</h2>
<div id="benchmark-datasets" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Benchmark datasets</h3>
<p>To evaluate how different monolingual and multilingual embeddings affect NER performance, we made use of large, well-annotated datasets for NER tasks. For Dutch and English, we used the data from the shared task of respectively the 2002 and 2003 Conference on Computational Natural Language Learning (CoNLL) <span class="citation">(Tjong Kim Sang <a href="#ref-tjongkimsang2002">2002</a>; Tjong Kim Sang and De Meulder <a href="#ref-tjongkimsang2003">2003</a>)</span>. These are widely used benchmarks datasets for Dutch and English NER. They consist of separate train, development and test sets of human-annotated, tokenized sentences from news articles from <a href="https://trec.nist.gov/data/reuters/reuters.html">Reuters Corpus</a> (English CoNLL 2003) and four editions of the Belgian newspaper “De Morgen” of the year 2000 (Dutch CoNLL2002). For French, the WikiNER dataset (wiki-3 version) was used <span class="citation">(Nothman et al. <a href="#ref-nothman2013">2013</a>)</span>. This dataset relied on automatic NER annotation algorithms based on Wikipedia’s article metadata as well as some additional heuristics, which has been shown to yield excellent-quality NER annotations. Because this dataset is much larger as compared to the CoNLL datasets, we used a random sample of 10 % of the entire dataset to approximately match the size of the CoNLL datasets (Table <a href="methods.html#tab:benchmark-ds">3.1</a>).</p>
<p>We maintained the train, development and test split of the original CoNLL datasets. However, the CoNLL2002 dataset contained a low number of relatively long sentences. Since some of the Transformer architectures used to obtain contextualized token representations (cf infra) have a limited input sequence length, the sentences exceeding 250 tokens were removed from the CoNLL2002 dataset for all experiments. This resulted in the removal of 4/15806 sentences from the training set and 1/5195 sentences in the test set.</p>
<p>All three datasets were pre-labelled with 4 different NER tags: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC), they were all converted to the same BIO2 format according to the original CoNLL2002 dataset <span class="citation">(Tjong Kim Sang <a href="#ref-tjongkimsang2002">2002</a>)</span>. This means that every line represented a word token and the NER tag, prefixed with either “B-” (“beginning”) or “I-” (“inside”) for respectively the first and subsequent tokens of entities spanning multiple tokens. Single-token entities were prefixed with “B-”, all other tokens were tagged with the label “O” (“other”). Different sentences were separated by a blank line. Indicators of document boundaries, such as the DOCSTART token in the CoNLL datasets, were removed. An example sentence from the Dutch CoNLL2002 dataset is given below.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="ex">Onder</span> O</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="ex">de</span> O</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="ex">bruuske</span> O</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="ex">tempoversnellingen</span> O</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="ex">van</span> O</a>
<a class="sourceLine" id="cb1-6" data-line-number="6"><span class="ex">Gilberto</span> B-PER</a>
<a class="sourceLine" id="cb1-7" data-line-number="7"><span class="ex">Simoni</span> I-PER</a>
<a class="sourceLine" id="cb1-8" data-line-number="8"><span class="ex">op</span> O</a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="ex">de</span> O</a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="ex">steilste</span> O</a>
<a class="sourceLine" id="cb1-11" data-line-number="11"><span class="ex">gedeelten</span> O</a>
<a class="sourceLine" id="cb1-12" data-line-number="12"><span class="ex">van</span> O</a>
<a class="sourceLine" id="cb1-13" data-line-number="13"><span class="ex">de</span> O</a>
<a class="sourceLine" id="cb1-14" data-line-number="14"><span class="ex">Pratonevoso</span> B-LOC</a>
<a class="sourceLine" id="cb1-15" data-line-number="15"><span class="ex">moest</span> O</a>
<a class="sourceLine" id="cb1-16" data-line-number="16"><span class="ex">Casagrande</span> B-PER</a>
<a class="sourceLine" id="cb1-17" data-line-number="17"><span class="ex">passen</span> O</a>
<a class="sourceLine" id="cb1-18" data-line-number="18"><span class="bu">.</span> <span class="ex">O</span></a></code></pre></div>
<p>In addition to these three monolingual dataset, we combined them into a single trilingual dataset to perform multilingual NER. For efficiency reasons, the combined dataset was obtained by randomly sampling one third of the sentences from each monolingual dataset (maintaining the same train, development and test splits). The specific datasets used here - together with all the code to reproduce the experiments - are made available on the GitHub repository of this thesis (<a href="https://github.com/arthur-arthur/NER">arthur-arthur/NER</a>). The English CoNLL2003 dataset requires a licence from <a href="https://trec.nist.gov/data/reuters/reuters.html">Reuters Corpus</a> (free for research purposes) and was therefore not included in the repository.</p>
<table>
<caption>
<span id="tab:benchmark-ds">Table 3.1: </span>Overview of the characteristics of the train, development (dev) and test benchmark datasets used to evaluate different monolingual and multilingual embeddings on monolingual and multilingual NER tasks. The WikiNER dataset was downsampled to roughly match the size of the Dutch and English datasets, the multilingual datasets was created by randomly sampling 1/3th of every monolingual dataset (maintaining the same train, dev and test splits).
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="2">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
# sentences
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
# tokens
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Train
</th>
<th style="text-align:right;">
Dev
</th>
<th style="text-align:right;">
Test
</th>
<th style="text-align:right;">
Train
</th>
<th style="text-align:right;">
Dev
</th>
<th style="text-align:right;">
Test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
En
</td>
<td style="text-align:left;">
CoNLL03
</td>
<td style="text-align:right;">
14041
</td>
<td style="text-align:right;">
3250
</td>
<td style="text-align:right;">
3453
</td>
<td style="text-align:right;">
203621
</td>
<td style="text-align:right;">
51362
</td>
<td style="text-align:right;">
46435
</td>
</tr>
<tr>
<td style="text-align:left;">
Nl
</td>
<td style="text-align:left;">
CoNLL02
</td>
<td style="text-align:right;">
15802
</td>
<td style="text-align:right;">
2895
</td>
<td style="text-align:right;">
5194
</td>
<td style="text-align:right;">
199969
</td>
<td style="text-align:right;">
37687
</td>
<td style="text-align:right;">
68466
</td>
</tr>
<tr>
<td style="text-align:left;">
Fr
</td>
<td style="text-align:left;">
WikiNER
</td>
<td style="text-align:right;">
10713
</td>
<td style="text-align:right;">
1190
</td>
<td style="text-align:right;">
1323
</td>
<td style="text-align:right;">
279729
</td>
<td style="text-align:right;">
34824
</td>
<td style="text-align:right;">
30991
</td>
</tr>
<tr>
<td style="text-align:left;">
Multi
</td>
<td style="text-align:left;">
Multilingual
</td>
<td style="text-align:right;">
13518
</td>
<td style="text-align:right;">
2444
</td>
<td style="text-align:right;">
3323
</td>
<td style="text-align:right;">
232173
</td>
<td style="text-align:right;">
40141
</td>
<td style="text-align:right;">
49444
</td>
</tr>
</tbody>
</table>
</div>
<div id="faktion-datasets" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Faktion datasets</h3>
<p>In addition to these well-validated benchmark datasets, similar experiments were performed on smaller “real-word” datasets that were obtained from Metamaze, a document annotation application developed by Faktion. The data consisted of text obtained with an in-house developed optical character recognition (OCR) pipeline on relatively poor quality scans of building plans. Hence, many of the tokens consisted of single characters or only punctuation, resulting in long sequences and many tokens with very little semantic information. The data were user-annotated within the Metamaze application, i.e. based on the location of the tokens in the document scans, not on the text output from the OCR pipeline itself. Given the limited input sequence length of some of the Transformer models, we reduced the average sequence length (i.e. tokens per document) by removing all tokens that consisted of punctuation only. Another difference with the benchmark datasets described earlier was the number of NER labels to predict: while there were 4 different entity labels in the CoNLL and WikiNER datasets (PER, LOC, ORG and MISC), there was only a single category (“naam_bouwplan”) in the Dutch Faktion dataset, and only one additional category (“naam_bouwplan_2”) in the French dataset, with approximately 95% of all entities belonging to a single category. A representative fragment of the OCR output for one of the documents (after preprocessing) is given below:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="ex">310</span> B-naam_bouwplan</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="ex">4</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="ex">Lewsrass</span> O</a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="ex">wes</span> O</a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="ex">BATIMENT</span> B-naam_bouwplan</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="ex">A</span> I-5e57</a>
<a class="sourceLine" id="cb2-7" data-line-number="7"><span class="ex">aren</span> O</a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="ex">seres</span> O</a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="ex">Meg</span> O</a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="ex">names</span> O</a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="ex">808</span> O</a>
<a class="sourceLine" id="cb2-12" data-line-number="12"><span class="ex">0e</span> O</a>
<a class="sourceLine" id="cb2-13" data-line-number="13"><span class="ex">side</span> O</a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="ex">panama</span> O</a>
<a class="sourceLine" id="cb2-15" data-line-number="15"><span class="ex">VERDIERING</span> B-naam_bouwplan</a>
<a class="sourceLine" id="cb2-16" data-line-number="16"><span class="ex">3</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-17" data-line-number="17"><span class="ex">-</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-18" data-line-number="18"><span class="ex">3</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-19" data-line-number="19"><span class="ex">1ewe</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-20" data-line-number="20"><span class="ex">E74</span> I-naam_bouwplan</a>
<a class="sourceLine" id="cb2-21" data-line-number="21"><span class="ex">A</span> O</a>
<a class="sourceLine" id="cb2-22" data-line-number="22"><span class="ex">0e</span> O</a>
<a class="sourceLine" id="cb2-23" data-line-number="23"><span class="ex">58</span> O</a>
<a class="sourceLine" id="cb2-24" data-line-number="24"><span class="ex">A0</span> O</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"><span class="ex">03</span> O</a></code></pre></div>
<p>Another important characteristic of these datasets was that none of them were strictly monolingual. The language was decided on the dominant language (Table <a href="methods.html#tab:faktion-ds">3.2</a>). A third - bilingual - dataset was created by merging the French and Dutch datasets. Given the characteristics of the datasets and, hence, the expected imprecision of the extra-sample error estimates, all experiments were repeated 5 times on independent random (80/20) splits of the data.</p>
<table>
<caption>
<span id="tab:faktion-ds">Table 3.2: </span>Number of sequences (documents) and tokens per train, development (dev) and test splits for the Dutch (Nl), French (Fr) and multilingual Faktion datasets. The reported values are the averages of the 5 independent random splits. Note that even the monolingual datasets contained bilingual documents, the language was chosen based on the dominant language in each dataset.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="2">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
# documents
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="3">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
# tokens
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
% bilingual
</th>
<th style="text-align:right;">
Train
</th>
<th style="text-align:right;">
Dev
</th>
<th style="text-align:right;">
Test
</th>
<th style="text-align:right;">
Train
</th>
<th style="text-align:right;">
Dev
</th>
<th style="text-align:right;">
Test
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Nl
</td>
<td style="text-align:left;">
58
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
2619
</td>
<td style="text-align:right;">
1062
</td>
<td style="text-align:right;">
789
</td>
</tr>
<tr>
<td style="text-align:left;">
Fr
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:right;">
91
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
9706
</td>
<td style="text-align:right;">
3273
</td>
<td style="text-align:right;">
3131
</td>
</tr>
<tr>
<td style="text-align:left;">
Multi
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
112
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
11976
</td>
<td style="text-align:right;">
4435
</td>
<td style="text-align:right;">
4171
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="word-embeddings" class="section level2">
<h2><span class="header-section-number">3.2</span> Word embeddings</h2>
<div id="static-and-task-specific-embeddings" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Static and task-specific embeddings</h3>
<p>As introduced in the previous chapter, there are a wide range of possible methods to obtain word representations that allow to perform NER. Static word embeddings like word2vec or fastText have had a significant impact on the performance of NER systems and have long been the default choice for many NLP tasks, including NER. They are still used today in combination with contextualized representations to provide state-of-the-art performance <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span>. We therefore included two types of static word embeddings, i.e. monolingual fastText word embeddings (English, Dutch and French) as well as monolingual BytePair and multilingual BytePair (mBytePair) embeddings (Table <a href="methods.html#tab:embeddings">3.4</a>), as discussed previously.</p>
<p>In addition, two types of task-specific representations were obtained. This included 300-dimensional one hot word type embeddings (OHEs) that were obtained by encoding all words in the vocabulary as one hot vectors and introducing an embedding layer into the network to obtain 300-dimensional word type representations<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>. In addition, we obtained 50-dimensional task-specific representations based on character-features<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>, as described in <span class="citation">(Lample et al. <a href="#ref-lample2016">2016</a>)</span>. The OHEs were included to confirm the well-established idea on the differences between learning word embeddings from scratch on the NER objective and using representations from pretrained LMs. The character-feature embeddings were included because they have been shown to improve NER performance when concatenated to other word representations <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>; Lample et al. <a href="#ref-lample2016">2016</a>)</span>. Both these task-specific representations were evaluated in the context of both monolingual and multilingual embeddings (Table <a href="methods.html#tab:embeddings">3.4</a>).</p>
</div>
<div id="contextualized-word-embeddings" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Contextualized word embeddings</h3>
<p>Even though static and/or task-specific representations have long been the default, today, all state-of-the-art NER systems rely on some form of contextualized word representation obtained from a pretrained neural LM. Below, we briefly discuss some neural network architectures and LMs that played an important role in the progress of the field during the recent years, how these models can be used to obtain multilingual word embeddings and which embedding types were used for the experiments presented in this thesis.</p>
<div id="recurrent-neural-networks" class="section level4">
<h4><span class="header-section-number">3.2.2.1</span> Recurrent Neural Networks</h4>
<p>Recurrent Neural Networks (RNN) are a class of neural network architectures that deal especially well with the sequential nature of natural language. In contrast to traditional neural networks, the hidden layer units do not simply pass their states along to input-output axis (i.e. from the input layer to the output layer), but pass the output of every unit to the next unit within that same layer. An example for a simple RNN LM is given in Figure <a href="methods.html#fig:rnnlm">3.1</a> (example taken from <span class="citation">(Manning <a href="#ref-manning2019">2019</a><a href="#ref-manning2019">b</a>)</span>). For this example, the RNN has to predict the next word in the sentence (“The students opened their”), i.e.:</p>
<p><span class="math display">\[\begin{equation}
P\left(w_5\mid the, \ students, \ opened, \ their\right) \notag
\end{equation}\]</span></p>
<p>First, words are represented as one-hot vectors <span class="math inline">\(\mathbf{w}_i \in \mathbb{R}^{|V|}\)</span> with <span class="math inline">\(i\)</span> indicating the position of the word in the sequence. Then, an embedding matrix <span class="math inline">\(\mathbf{E} \in \mathbb{R}^{d \times |V|}\)</span> is used to obtain static <span class="math inline">\(d\)</span>-dimensional word embeddings like word2vec <span class="citation">(Mikolov et al. <a href="#ref-mikolov2013">2013</a>)</span>.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{e}_i = \mathbf{E}\mathbf{w}_i \notag
\end{equation}\]</span></p>
<p>The hidden state for every unit <span class="math inline">\(\mathbf{h}_i\)</span> is computed as a linear combination of the word vector for the <span class="math inline">\(i\)</span>-th word in the sequence, <span class="math inline">\(\mathbf{e}_i\)</span>, as well as the hidden state from the previous unit <span class="math inline">\(\mathbf{h}_{i-1}\)</span>. This is passed through a nonlinear activation function <span class="math inline">\(\sigma\)</span>, just like a regular feed-forward neural network:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{h}_i =  \sigma (\mathbf{W}_h \mathbf{h}_{i-1} + \mathbf{W}_e \mathbf{e}_{i}) \notag
\end{equation}\]</span></p>
<p>Lastly, a linear combination of the last hidden state <span class="math inline">\(\mathbf{h}_{4}\)</span> (that encodes information on the last input word, as well as the entire past context) is passed through a softmax activation function <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016">2016</a>)</span> (i.e. essentially performing multinomial logistic regression) to predict a probability distribution over the vocabulary (<span class="math inline">\(\mathbf{\hat{y}}_4 \in \mathbb{R}^{|V|}\)</span>) for the last word <span class="math inline">\(\mathbf{w}_5\)</span>, i.e. more generally:</p>
<p><span class="math display">\[\begin{equation}
\mathbf{\hat{y}}_i =  softmax (\mathbf{W}_o \mathbf{h}_{i}) \notag
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\mathbf{W}_e\)</span>, <span class="math inline">\(\mathbf{W}_h\)</span>, <span class="math inline">\(\mathbf{W}_o\)</span> being the weight matrices to be learned by the network (biases are omitted). Introducing the matrix <span class="math inline">\(\mathbf{W}_h\)</span> allows the network to learn how to make use of this past context (encoded in <span class="math inline">\(\mathbf{h}_{i-1}\)</span>) while computing the output of its current state. Since the weight matrix <span class="math inline">\(\mathbf{W}_h\)</span> is shared across all timesteps, the length of the input sequence does not increases the model size in terms of parameters. While the aforementioned <span class="math inline">\(n\)</span>-gram language models are constrained by the assumption that a given word only depends on a limited number of previous words, RNNs can effectively consider <em>all</em> preceding words, making them well-suited for the LM objective.</p>

<div class="figure" style="text-align: center"><span id="fig:rnnlm"></span>
<img src="images/RNN_LM.pdf" alt="A schematic representation of a simple RNN LM with a single hidden layer. The output is passed through a softmax layer to generate a probability distribution over the entire vocabulary. The model is trained in a completely unsupervised way through minimizing the cross-entropy loss, i.e. the negative log probability that the model assigned to the correct word. This conceptually very simple idea forms the foundation of many state-of-the-art neural LMs that are used today. Adapted from (Manning 2019a)." width="75%" />
<p class="caption">
Figure 3.1: A schematic representation of a simple RNN LM with a single hidden layer. The output is passed through a softmax layer to generate a probability distribution over the entire vocabulary. The model is trained in a completely unsupervised way through minimizing the cross-entropy loss, i.e. the negative log probability that the model assigned to the correct word. This conceptually very simple idea forms the foundation of many state-of-the-art neural LMs that are used today. Adapted from <span class="citation">(Manning <a href="#ref-manning2019a">2019</a><a href="#ref-manning2019a">a</a>)</span>.
</p>
</div>
<p>Unfortunately, in practice, RNN tend to encode relatively local information in their hidden states. The backpropagation step during training involves computing gradients and multiplying these (since the loss at a given time step is a function of the previous hidden state in the sequence). When the number of units in the sequence increases, these sequential multiplications can easily result in very large or very small numbers, a phenomenon commonly referred to as exploding or vanishing gradients, respectively <span class="citation">(Bengio, Frasconi, and Simard <a href="#ref-bengio1993">1993</a>)</span>. Since the model parameters are iteratively updated according to this gradient, this can cause the algorithm to diverge or dramatically slow down the learning process.</p>
</div>
<div id="long-short-term-memory-models" class="section level4">
<h4><span class="header-section-number">3.2.2.2</span> Long Short Term Memory models</h4>
<p>Long Short Term Memory (LSTM) networks involve an additional level of architectural complexity and expand the parameter space, but provide an elegant solution to the aforementioned vanishing gradient issue. They learn how to actively manage contextual information in a so-called “memory-cell” - an additional vector that encodes historical information. The model is allowed to learn how to efficiently manage this historical information by parametrizing it with its own set of weights that control the flow of information in and out of the units of the hidden layers. This renders LSTMs better at handling contextual information and encoding long-range dependencies in its hidden states, as compared to simple RNNs <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016">2016</a>)</span>. LSTMs are widely used for many applications, including classification tasks such as NER. In fact, one of the best-performing NER classifier models today is based on a LSTM and this specific architecture was used for the experiments presented in this thesis. A schematic overview of this architecture is given in Figure <a href="methods.html#fig:bilstmcrf">3.4</a>. In addition, the ability of LSTMs to effectively model very long-range dependencies generally translates into excellent performance on the LM task. Today, many deep neural LMs consist of stacked layers of sequential LSTM units, and are often optimized for two objectives: to predict a word given the previous words, and to predict a word given the next words. Combining these forward and backward LMs yield so-called BiLSTM LMs that are used in many NLP applications, such as text autocompletion tasks, machine translation or speech recognition. In addition, their hidden states provide excellent contextualized word embeddings, as dicussed below.</p>
</div>
<div id="taglm" class="section level4">
<h4><span class="header-section-number">3.2.2.3</span> TagLM</h4>
<p>One of the first architectures to effectively exploit the hidden states of a BiLSTM LM as contextual word vectors to train a NER classifier was the TagLM model <span class="citation">(Peters et al. <a href="#ref-peters2017">2017</a>)</span>. In short, the TagLM model made use of a large BiLSTM that was pre-trained using a large corpus of text on a LM objective. A second neural classifier was trained to perform NER. Instead of just feeding static word embeddings (e.g. word2vec or fastText) into this classifier, or training task-specific representations from scratch on the NER objective, they fed their input sequences into the pre-trained LM and extracted the hidden states. By feeding both static word embeddings as well as the concatenated forward and backward hidden states of the LM into the NER classifier, they achieved state-of-the-art performance on the NER task (Figure <a href="methods.html#fig:taglm">3.2</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:taglm"></span>
<img src="images/TagLM.png" alt="Schematic overview of the TagLM model. The NER task is based on both static word embeddings as well as contextualized word representations from the hidden states of a pre-trained bidirectional recurrent language model. Taken from (Peters et al. 2017)" width="85%" />
<p class="caption">
Figure 3.2: Schematic overview of the TagLM model. The NER task is based on both static word embeddings as well as contextualized word representations from the hidden states of a pre-trained bidirectional recurrent language model. Taken from <span class="citation">(Peters et al. <a href="#ref-peters2017">2017</a>)</span>
</p>
</div>

</div>
<div id="elmo" class="section level4">
<h4><span class="header-section-number">3.2.2.4</span> ELMo</h4>
<p>The ELMo (Embeddings from Language Models) architecture was an improved version of the TagLM model that achieved incredible performance on many NLP-tasks, including NER. ELMo is based on a deep, multi-layer BiLSTM LM to obtain context- and task-specific word representations from the hidden states of the LM layers <span class="citation">(Peters et al. <a href="#ref-peters2018">2018</a>)</span>. The excellent performance was partly achieved through representing words as a weighted average of the internal representations of the LM. It turned out that by introducing these weights as trainable parameters of the downstream task, the downstream model can make optimal use of the semantic and syntactic information that are encoded in the different layers of the LM. ELMo embeddings were able to improve the state-of-the-art of virtually all possible NLP tasks with a very significant margin.</p>
</div>
<div id="flair" class="section level4">
<h4><span class="header-section-number">3.2.2.5</span> Flair</h4>
<p>Another succesful approach to obtain contextual word embeddings was the Flair architecture, developed by Alan Akbik and colleagues in 2018 <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span>. The general idea of Flair is - again - to obtain contextual word representations from a large, pre-trained language model that can subsequently be used to train a supervised NLP task such as NER.</p>
<p>In contrast to ELMo, the authors used a character-based BiLSTM LM, i.e. the model was trained to predict a character given the previous characters in the sequence (and <em>vice versa</em>, since it was a bidirectional LM) - without given any explicit notion of words. When a lot of training data is provided, these models have been shown to succesfully internalize linguistic concepts such as words, sentences, grammar, punctuation and even more complex, long-range linguistic dependencies <span class="citation">(Graves <a href="#ref-graves2014">2014</a>; Sutskever, Vinyals, and Le <a href="#ref-sutskever2014">2014</a>)</span>. Advantages of using a character-level language model are the improved handling of misspelled and rare words. In addition, since the “vocabulary” of these models is fixed and very compact (characters instead of words), these models are very efficient to train, independent from tokenization. By concatenating the internal states of the language model from both the forward and backward LSTM, they obtained context-specific word vectors, as illustrated in Figure <a href="methods.html#fig:flairLM">3.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:flairLM"></span>
<img src="images/flair_LM.png" alt="The character-based bi-directional Flair LM yields word representations (\(\mathbf{r}\)) that contain information from the word itself, as well as from surrounding characters in the sentence, thus encoding information from the entire sequence. Taken from (Akbik, Blythe, and Vollgraf 2018)." width="100%" />
<p class="caption">
Figure 3.3: The character-based bi-directional Flair LM yields word representations (<span class="math inline">\(\mathbf{r}\)</span>) that contain information from the word itself, as well as from surrounding characters in the sentence, thus encoding information from the entire sequence. Taken from <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span>.
</p>
</div>

<p>They proposed an architecture for NER that was based on their character-based LM and a downstream BiLSTM-CRF classifier. The authors experimented with concatenating their contextual character-based word representations with ‘static’ GloVe word embeddings as well as the task-specific character-feature embeddings discussed previously. They demonstrated that a stacked embedding containing the proposed character-level contextualized word embeddings as well as the trainable character-feature embeddings and static GloVe embeddings resulted in state-of-the-art performance on the CoNLL 2003 NER task <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>)</span>.</p>
<p>In addition to training the aforementioned language model, the authors also released an open source PyTorch-based NLP library <span class="citation">(Akbik et al. <a href="#ref-akbik2019b">2019</a>)</span> that supports a wide range of pre-trained embeddings and LMs. The high training efficiency of the character-based LMs is one of the main reasons that several Flair LMs have been pretrained in different languages. For the experiments reported in this thesis, we used the original (English) Flair model described here, as well as the Dutch, French and multilingual versions in different configurations with other static or task/specific embeddings, as explained below (Table <a href="methods.html#tab:embeddings">3.4</a>). The Dutch, French and multilingual versions of Flair were trained by members of the community and made freely available in the Flair library<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>.</p>
</div>
<div id="bert" class="section level4">
<h4><span class="header-section-number">3.2.2.6</span> BERT</h4>
<p>One of the main factors driving improvement in the performance of deep neural LMs is their scale or the amount of training data they are able to learn from. Neural language modelling is an unsupervised task which means that they are not restricted by the scarcity of human-annotated data. Wikipedia or news articles provide an enormous amount of qualitative training data, which means that much of the progress in the last few years can be attributed to the efficiency and scalability of the training algorithms. Even though RNNs like LSTMs provide excellent LMs, they have a major disadvantage: the input of each hidden state relies on the output of the preceding hidden state, making them very slow to train and difficult to parallelize. A major breakthrough with respect to training efficiency of neural LMs was the development of the so-called Transformer architecture in 2017 <span class="citation">(Vaswani et al. <a href="#ref-vaswani2017">2017</a>)</span>. For an overview of the Transformer architecture we refer to <a href="http://jalammar.github.io/">Jay Alammar’s</a> excellent post <a href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a>. In short, the Transformer architecture completely omits the recurrent nature of RNNs, while keeping the ability to “memorize” contextual information. This was achieved through the introduction of an encoder-decoder architecture (typically used in machine translation tasks) and a so-called Multi-Head Attention block, a self-attention mechanism that keeps track of the relationship between input and output, allowing the netwerk to consider specific parts of the input sequence (that can be far away) to improve its performance on the LM objective. A crucial advantage of omiting the recurrency is the fact that training can be parallelized. This facilitates scaling and this is essentially what drove the steady increase in the scale of neural LMs during the past few years.</p>
<p>A very succesful application of the Transformer architecture was Google’s BERT (Bidirectional Encoder Representations from Transformers) <span class="citation">(Devlin et al. <a href="#ref-devlin2019">2019</a>)</span>. BERT was innovative in the sense that it used a bidirectional masked LM objective. Instead of predicting a word given the previous words (or <em>vice versa</em> for a backward LM objective), they randomly masked 15 % of the words in the corpus. By training the model to predict these masked words from the entire context, the model could jointly learn from both the left and right side of the target word. This is in contrast to a BiLSTM, in the sense that both the forward and backward LM learn independently from either history or future, as illustrated in Fig. <a href="methods.html#fig:bilstmcrf">3.4</a>. The contextualized representations learned by BERT have been used extensively in NLP - often through adding a task-specific layer on top and fine-tuning the entire BERT model on a specific NLP task. BERT or some of its many successors (ALBERTA, RoBERTa, XLM, etc…) are able to exceed human-like performance on high-level NLP tasks like question answering<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. This illustrates that these models are able to internalize a large amount of linguistic knowledge - in a completely unsupervised way - and achieve a level of natural language understanding that - for some tasks - comes very close or even exceeds the level of natural language understanding that humans are able to acquire throughout their lives.</p>
<p>For the experiments presented in this report, we used, for English, the <a href="https://huggingface.co/bert-base-cased">case-sensitive BERT base model</a> (<a href="https://github.com/huggingface/transformers">Huggingface Transformer library</a><a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> reference: <code>bert-base-cased</code>), the <a href="https://huggingface.co/camembert-base">CamemBERT</a> (<code>camembert-base</code>) model for French <span class="citation">(Martin et al. <a href="#ref-martin2019">2019</a>)</span> and <a href="https://huggingface.co/wietsedv/bert-base-dutch-cased">BERTje</a> (<code>wietsedv/bert-base-dutch-cased</code>) for Dutch <span class="citation">(de Vries et al. <a href="#ref-devries2019">2019</a>)</span>. <a href="https://huggingface.co/bert-base-multilingual-cased">Multilingual BERT (mBERT)</a> (<code>bert-base-multilingual-cased</code>) was used to obtain multilingual embeddings. We did not perform any fine-tuning of the weights of the BERT models, but obtained contextualized token representations by extracting the activations from the top layer of the Transformer models. Since the BERT models are based on subword tokenization and the NER model operates on the token-level, we used the representations of the first subtoken as token-level embeddings, similar to the feature-based approach described in the original BERT paper <span class="citation">(Devlin et al. <a href="#ref-devlin2019">2019</a>)</span>. The dimensions of the obtained embeddings from the different BERT-models are given in Table <a href="methods.html#tab:emb-dim">3.3</a>.</p>
<table>
<caption>
<span id="tab:emb-dim">Table 3.3: </span>Overview of the dimensions of all English (En), French (Fr), Dutch (Nl) and multilingual (Multi) word vectors used in this thesis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Embedding
</th>
<th style="text-align:left;">
En
</th>
<th style="text-align:left;">
Fr
</th>
<th style="text-align:left;">
Nl
</th>
<th style="text-align:left;">
Multi
</th>
</tr>
</thead>
<tbody>
<tr grouplength="2">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>Static</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
fastText
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
BytePair
</td>
<td style="text-align:left;">
100
</td>
<td style="text-align:left;">
100
</td>
<td style="text-align:left;">
100
</td>
<td style="text-align:left;">
3672
</td>
</tr>
<tr grouplength="2">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>Contextualized</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Flair
</td>
<td style="text-align:left;">
4096
</td>
<td style="text-align:left;">
2048
</td>
<td style="text-align:left;">
4096
</td>
<td style="text-align:left;">
6144
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
BERT
</td>
<td style="text-align:left;">
768
</td>
<td style="text-align:left;">
768 
</td>
<td style="text-align:left;">
768 
</td>
<td style="text-align:left;">
4440
</td>
</tr>
<tr grouplength="2">
<td colspan="5" style="border-bottom: 1px solid;">
<strong>Task-specific</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
One hot word type
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
300
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Character
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;">
50
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup>a</sup> CamemBERT; <sup>b</sup> BERTje
</td>
</tr>
</tfoot>
</table>
<div style="page-break-after: always;"></div>
<table class="table" style="font-size: 9px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:embeddings">Table 3.4: </span>Overview of the different embeddings evaluated in the context of English, Dutch, French and multilingual NER. The columns indicate the type of LM used to obtain contextualized embeddings (if any). The different configurations w.r.t. static and/or task-specific embedding types are indicated by the different rows. 
</caption>
<thead>
<tr>
<th style="text-align:left;">
No contextualized embeddings
</th>
<th style="text-align:left;">
BERT
</th>
<th style="text-align:left;">
Flair
</th>
<th style="text-align:left;">
BERT + Flair
</th>
</tr>
</thead>
<tbody>
<tr grouplength="6">
<td colspan="4" style="border-bottom: 1px solid;">
<strong>English</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
n.a.
</td>
<td style="text-align:left;">
BERT
</td>
<td style="text-align:left;">
Flair
</td>
<td style="text-align:left;">
BERT + Flair
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Char
</td>
<td style="text-align:left;">
BERT + Char
</td>
<td style="text-align:left;">
Flair + Char
</td>
<td style="text-align:left;">
BERT + Flair + Char
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
OHE
</td>
<td style="text-align:left;">
BERT + OHE
</td>
<td style="text-align:left;">
Flair + OHE
</td>
<td style="text-align:left;">
BERT + Flair + OHE
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
BPEmb (En)
</td>
<td style="text-align:left;">
BERT + BPEmb (En)
</td>
<td style="text-align:left;">
Flair + BPEmb (En)
</td>
<td style="text-align:left;">
BERT + Flair + BPEmb (En)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
fastT (En)
</td>
<td style="text-align:left;">
BERT + fastT (En)
</td>
<td style="text-align:left;">
Flair + fastT (En)
</td>
<td style="text-align:left;">
BERT + Flair + fastT (En)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
All 
</td>
<td style="text-align:left;">
BERT + All 
</td>
<td style="text-align:left;">
Flair + All 
</td>
<td style="text-align:left;">
BERT + Flair + All 
</td>
</tr>
<tr grouplength="6">
<td colspan="4" style="border-bottom: 1px solid;">
<strong>Dutch</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
n.a.
</td>
<td style="text-align:left;">
BERTje
</td>
<td style="text-align:left;">
Flair (Nl)
</td>
<td style="text-align:left;">
BERTje + Flair (Nl)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Char
</td>
<td style="text-align:left;">
BERTje + Char
</td>
<td style="text-align:left;">
Flair (Nl) + Char
</td>
<td style="text-align:left;">
BERTje + Flair (Nl) + Char
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
OHE
</td>
<td style="text-align:left;">
BERTje + OHE
</td>
<td style="text-align:left;">
Flair (Nl) + OHE
</td>
<td style="text-align:left;">
BERTje + Flair (Nl) + OHE
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
BPEmb (Nl)
</td>
<td style="text-align:left;">
BERTje + BPEmb (Nl)
</td>
<td style="text-align:left;">
Flair (Nl) + BPEmb (Nl)
</td>
<td style="text-align:left;">
BERTje + Flair (Nl) + BPEmb (Nl)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
fastT (Nl)
</td>
<td style="text-align:left;">
BERTje + fastT (Nl)
</td>
<td style="text-align:left;">
Flair (Nl) + fastT (Nl)
</td>
<td style="text-align:left;">
BERTje + Flair (Nl) + fastT (Nl)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
All 
</td>
<td style="text-align:left;">
BERTje + All 
</td>
<td style="text-align:left;">
Flair (Nl) + All 
</td>
<td style="text-align:left;">
BERTje + Flair (Nl) + All 
</td>
</tr>
<tr grouplength="6">
<td colspan="4" style="border-bottom: 1px solid;">
<strong>French</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
n.a.
</td>
<td style="text-align:left;">
CamemBERT
</td>
<td style="text-align:left;">
Flair (Fr)
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Char
</td>
<td style="text-align:left;">
CamemBERT + Char
</td>
<td style="text-align:left;">
Flair (Fr) + Char
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr) + Char
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
OHE
</td>
<td style="text-align:left;">
CamemBERT + OHE
</td>
<td style="text-align:left;">
Flair (Fr) + OHE
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr) + OHE
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
BPEmb (Fr)
</td>
<td style="text-align:left;">
CamemBERT + BPEmb (Fr)
</td>
<td style="text-align:left;">
Flair (Fr) + BPEmb (Fr)
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr) + BPEmb (Fr)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
fastT (Fr)
</td>
<td style="text-align:left;">
CamemBERT + fastT (Fr)
</td>
<td style="text-align:left;">
Flair (Fr) + fastT (Fr)
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr) + fastT (Fr)
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
All 
</td>
<td style="text-align:left;">
CamemBERT + All 
</td>
<td style="text-align:left;">
Flair (Fr) + All 
</td>
<td style="text-align:left;">
CamemBERT + Flair (Fr) + All 
</td>
</tr>
<tr grouplength="5">
<td colspan="4" style="border-bottom: 1px solid;">
<strong>Multilingual</strong>
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
n.a.
</td>
<td style="text-align:left;">
mBERT
</td>
<td style="text-align:left;">
mFlair
</td>
<td style="text-align:left;">
mBERT + mFlair
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
Char
</td>
<td style="text-align:left;">
mBERT + Char
</td>
<td style="text-align:left;">
mFlair + Char
</td>
<td style="text-align:left;">
mBERT + mFlair + Char
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
OHE
</td>
<td style="text-align:left;">
mBERT + OHE
</td>
<td style="text-align:left;">
mFlair + OHE
</td>
<td style="text-align:left;">
mBERT + mFlair + OHE
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
mBPEmb
</td>
<td style="text-align:left;">
mBERT + mBPEmb
</td>
<td style="text-align:left;">
mFlair + mBPEmb
</td>
<td style="text-align:left;">
mBERT + mFlair + mBPEmb
</td>
</tr>
<tr>
<td style="text-align:left; padding-left: 2em;" indentlevel="1">
All 
</td>
<td style="text-align:left;">
mBERT + All 
</td>
<td style="text-align:left;">
mFlair + All 
</td>
<td style="text-align:left;">
mBERT + mFlair + All 
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border: 0;" colspan="100%">
<sup>a</sup> Char + OHE + BPEmb (En) + fastT (En) <sup>b</sup> Char + OHE + BPEmb (Nl) + fastT (Nl) <sup>c</sup> Char + OHE + BPEmb (Fr) + fastT (Fr) <sup>d</sup> Char + OHE + mBPEmb
</td>
</tr>
</tfoot>
</table>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
<div id="ner-classifier" class="section level2">
<h2><span class="header-section-number">3.3</span> NER classifier</h2>
<p>NER is essentially a classification problem. The sequential nature of languange renders some algorithms more suited for the task than others. Today, state-of-the-art systems for sequence labeling tasks like NER are typically based on a BiLSTM with a Conditional Random Field (CRF) decoding layer <span class="citation">(Lafferty, Mccallum, and Pereira <a href="#ref-lafferty2001">2001</a>)</span>, as illustrated in Figure <a href="methods.html#fig:bilstmcrf">3.4</a>.</p>
<p>For a given sequence of words <span class="math inline">\((w_1, \ ..., \ w_m)\)</span>, the BiLSTM provides a representations <span class="math inline">\(\mathbf{H} = (\mathbf{h}_1, ... , \ \mathbf{h}_m)\)</span> with <span class="math inline">\(\mathbf{h}_i = [\mathbf{h}_i^f ; \ \mathbf{h}_i^b]\)</span> i.e. the concatenated output states of the <span class="math inline">\(i\)</span>-th unit of the forward and backward LSTM (Figure <a href="methods.html#fig:bilstmcrf">3.4</a>). One could simply use the representation <span class="math inline">\(\mathbf{h}_i\)</span> as a feature vector to predict the tag for the <span class="math inline">\(i\)</span>-th output, i.e. model the conditional distributions <span class="math inline">\(P(y_i \ | \ \mathbf{h}_i)\)</span> independently for <span class="math inline">\(i=(1, \ ..., \ m)\)</span> by passing the BiLSTM output states through a nonlinear softmax activation function <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-goodfellow2016">2016</a>)</span>. Alternatively, one could model the conditional distribution of an entire sequence of output labels <span class="math inline">\(\mathbf{y} = (y_1, \ ..., \ y_m)\)</span> given a sequence of word representations <span class="math inline">\(\mathbf{H}\)</span>, i.e. <span class="math inline">\(P(\mathbf{y} \ | \ \mathbf{H}) = P(y_1, \ ..., \ y_m \ | \ \mathbf{h}_1, \ ..., \ \mathbf{h}_m )\)</span>. By using this so-called CRF over all possible tag sequences and jointly model the full sequence of labels for some input sequence, the model is able to consider the strong dependencies between neighboring tags (e.g. B-LOC cannot be followed by I-PER), which has been shown to generally improve performance on NER tasks <span class="citation">(Huang, Xu, and Yu <a href="#ref-huang2015">2015</a>; Reimers and Gurevych <a href="#ref-reimers2017">2017</a>)</span>. The parametrization and estimation procedure of the CRF in the context of NER is described in <span class="citation">(Ma and Hovy <a href="#ref-ma2016">2016</a>)</span>. For the experiments reported in this thesis, we used the BiLSTM-CRF implementation from the <a href="https://github.com/flairNLP/flair">Flair NLP library</a> (version 0.4.5 / 0.5.0). More information on this specific implementation can be found in <span class="citation">(Akbik, Blythe, and Vollgraf <a href="#ref-akbik2018">2018</a>; Huang, Xu, and Yu <a href="#ref-huang2015">2015</a>)</span>.</p>

<div class="figure" style="text-align: center"><span id="fig:bilstmcrf"></span>
<img src="images/bilstmcrf.pdf" alt="A simplified representation of the BiLSTM-CRF architecture that was used to perform NER for the experiments reported in this thesis. The input of the BiLSTM are either static, task-specific or contextualized word embeddings (or a concatenation of different embedding types). The forward (\(\mathbf{h}_i^f\)) and backward (\(\mathbf{h}_i^b\)) output states of the BiLSTM are concatenated into a contextualized representation (\(\mathbf{h}_i = [\mathbf{h}_i^f; \mathbf{h}_i^b]\)) that is used to predict the correct tag sequence using a Conditional Random Field (CRF), as described in the text." width="100%" />
<p class="caption">
Figure 3.4: A simplified representation of the BiLSTM-CRF architecture that was used to perform NER for the experiments reported in this thesis. The input of the BiLSTM are either static, task-specific or contextualized word embeddings (or a concatenation of different embedding types). The forward (<span class="math inline">\(\mathbf{h}_i^f\)</span>) and backward (<span class="math inline">\(\mathbf{h}_i^b\)</span>) output states of the BiLSTM are concatenated into a contextualized representation (<span class="math inline">\(\mathbf{h}_i = [\mathbf{h}_i^f; \mathbf{h}_i^b]\)</span>) that is used to predict the correct tag sequence using a Conditional Random Field (CRF), as described in the text.
</p>
</div>
<p>For the benchmark datasets (CoNLL2002, CoNLL2003, WikiNER), the model hyperparameters were based on the <a href="https://github.com/flairNLP/flair/blob/master/resources/docs/EXPERIMENTS.md">best known configuration</a> for the CoNLL tasks as reported by the authors of the <a href="https://github.com/flairNLP/flair">Flair library</a>. In what follows we provide the parameter names of the <code>SequenceTagger</code> and <code>ModelTrainer</code> classes of the Flair library. A BiLSTM-CRF model with a forward and backward hidden layer of 256 units (<code>hidden_size</code>) was used. Training was done using stochastic gradient descent with a <code>mini_batch_size</code> of 32 instances, i.e. at each iteration, 32 sentences from the training set were propagated through the network (sentences were shuffled at each epoch). The initial <code>learning_rate</code> was set to 0.1. Early stopping criteria were specified by setting <code>patience</code> = 3 and <code>anneal_factor</code> = 0.5 which means that the learning rate was multiplied by a factor 0.5 when the validation F1-score did not improve for 3 consecutive epochs. When the learning rate dropped below the <code>min_learning_rate</code> of 0.0001 or after 100 epochs (as specified by the <code>max_epochs</code> parameter), the training procedure was stopped. The final model was refitted on the training and validation set and performance was estimated through computing the micro-average precision, recall and F1-score (cf infra) on the held-out test set. The approach was similar for the Faktion datasets, with some minor changes in terms of the NER classifier architecture and the early stopping criteria to optimize performance for these datasets. The hyperparameters for both the benchmark and Faktion datasets are summarized in table <a href="methods.html#tab:params">3.5</a>.</p>
<table>
<caption>
<span id="tab:params">Table 3.5: </span>Overview of the hyperparameter configuration for all NER experiments on the benchmark datasets and Faktion datasets. The terminology used here is consistent with the parameters of the <code>SequenceTagger</code> and <code>ModelTrainer</code> classes of the <a href="https://github.com/flairNLP/flair">Flair NLP library</a>. Further details can be found in the <a href="https://github.com/flairNLP/flair/tree/master/resources/docs">documentation of the Flair NLP library</a> or in the <a href="https://github.com/arthur-arthur/NER/">GitHub repository</a> of this thesis.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Parameter
</th>
<th style="text-align:left;">
Benchmark
</th>
<th style="text-align:left;">
Faktion
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
hidden_size
</td>
<td style="text-align:left;">
256
</td>
<td style="text-align:left;">
32
</td>
</tr>
<tr>
<td style="text-align:left;">
learning_rate
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
</tr>
<tr>
<td style="text-align:left;">
anneal_factor
</td>
<td style="text-align:left;">
0.5
</td>
<td style="text-align:left;">
0.8
</td>
</tr>
<tr>
<td style="text-align:left;">
patience
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
min_learning_rate
</td>
<td style="text-align:left;">
1e-04
</td>
<td style="text-align:left;">
1e-04
</td>
</tr>
<tr>
<td style="text-align:left;">
mini_batch_size
</td>
<td style="text-align:left;">
32
</td>
<td style="text-align:left;">
8
</td>
</tr>
<tr>
<td style="text-align:left;">
max_epochs
</td>
<td style="text-align:left;">
100
</td>
<td style="text-align:left;">
100
</td>
</tr>
</tbody>
</table>

<p>All models were trained using the cloud computational environment of <a href="https://www.kaggle.com">Kaggle</a>, either on a CPU (4 cores, 16 Gb RAM) or on a single NVIDIA Tesla P100 GPU (2 CPU cores, 16 Gb RAM) (Python 3.7.6). We made use of the Kaggle API to automatically configure Python scripts for each experiment and push it to the Kaggle environment from a simple command line interface. All code and information to reproduce the experiments can be found on the <a href="https://github.com/arthur-arthur/NER">Github repository (arthur-arthur/NER)</a> of this thesis.</p>
</div>
<div id="ner-evaluation" class="section level2">
<h2><span class="header-section-number">3.4</span> NER evaluation</h2>
<p>According to the <a href="https://www.clips.uantwerpen.be/conll2000/chunking/output.html">CoNLL guidelines</a>, evaluation of NER is performed at the entity-level, not at the token or word-level (to clarify this, an example is provided at the end of this paragraph). This has some consequences on the type of metrics that are used to evaluate NER performance, as well as on their interpretation.</p>
<p>Firstly, since entity-level evaluation is based on the sets of predicted and true entities, there is no clear definition of a “true negative” entity <span class="citation">(Esuli and Sebastiani <a href="#ref-esuli2010">2010</a>)</span>. Hence, evaluation metrics that are a function of the number of true negatives (ROC analysis, classification accuracy, Matthews Correlation Coefficient,…) are not very meaningful in this context. For this reason, NER performance is commonly evaluated in terms of entity-level precision, recall and F1-score. Precision gives the proportion of all predicted entities that are indeed correct:</p>
<p><span class="math display">\[
P = \frac{TP}{TP + FP}
\]</span>
with TP and FP being true positives and false positives, respectively. Recall indicates how much of the entities in the dataset were correctly predicted:</p>
<p><span class="math display">\[
R = \frac{TP}{TP + FN}
\]</span>
with FN being false negatives. Note that one can easily design a system that achieves high recall by predicting all instances as entities such that <span class="math inline">\(FN=0\)</span>. This will typically result in a high number of false positives and, hence, a low precision. Conversely, when only a single entity from the entire dataset is correctly predicted, <span class="math inline">\(P=1\)</span>. In that case, the number of false negatives is typically high and, hence, recall will be low. The goal is therefore to find a system that has an optimal balance between precision and recall, which is commonly expressed in terms of the F1-score - the gold standard evaluation metric for NER <span class="citation">(Tjong Kim Sang and De Meulder <a href="#ref-tjongkimsang2003">2003</a>)</span>. The F1-score is defined as the harmonic mean of precision and recall:</p>
<p><span class="math display">\[
F_1 = \frac{2PR}{P + R}
\]</span></p>
<p>To illustrate how gold standard entity-level evaluation of NER systems is performed, we provide the following example sentence (for simplicity, the example includes only a single entity class, this extends naturally to cases with multiple classes):</p>
<ul>
<li><strong>Gold sentence: </strong><br />
“From Bormio [B-LOC] to Prato [B-LOC] allo [I-LOC] Stelvio [I-LOC]”.</li>
<li><strong>Predicted sentence: </strong><br />
“From Bormio [B-LOC] to Prato [B-LOC] allo Stelvio [B-LOC]”</li>
</ul>
<p>The set of true entities thus includes two instances: “Bormio” and “Prato allo Stelvio”. The NER system correctly identified the token “Bormio” as a location, i.e. this is a true positive (TP) prediction. The system failed to correctly predict the segment “Prato allo Stelvio” as a location spanning three tokens, i.e. this is a false negative (FN) prediction. In addition, the NER tagger predicted “Prato [B-LOC]” and “Stelvio [B-LOC]” as two separate entities, while these were not in the set of true entities. Hence, these are two false positive (FP) predictions. Overall, the precision for this example is <span class="math inline">\(\frac{1}{1+2} = 0.33\)</span>, the recall is <span class="math inline">\(\frac{1}{1 + 1} = 0.5\)</span> and the F1-score is <span class="math inline">\(\frac{2 \times 0.33 \times 0.50}{0.33 + 0.50} = 0.40\)</span>.</p>
<p>When there are multiple entity classes to predict (which is the case for most NER systems), the overall performance of the system can be expressed in terms of the macro-average or micro-average precision, recall or F1-scores. Macro-averaging involves simply averaging the scores per entity class, while the micro-average refers to the weighted average of the class scores, with the weights being determined by the number of instances per class. Since the latter takes the class balance into account. NER performance is typically evaluated and reported in terms of the micro-average F1-score. We will adhere to this convention and report the micro-average precision, recall and F1-scores for the NER experiments performed during this thesis. For the benchmark datasets, only a single training run was performed. When multiple training runs were performed (i.e. for the Faktion datasets), the F1-scores were reported as mean ± standard error of the mean (sem) for <span class="math inline">\(k = 5\)</span> runs. All scores reported in this thesis were computed on the held-out test set.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-akbik2019b">
<p>Akbik, Alan, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. “FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP.” In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>, 54–59. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-4010">https://doi.org/10.18653/v1/N19-4010</a>.</p>
</div>
<div id="ref-akbik2018">
<p>Akbik, Alan, Duncan Blythe, and Roland Vollgraf. 2018. “Contextual String Embeddings for Sequence Labeling.” In <em>Proceedings of the 27th International Conference on Computational Linguistics</em>, 1638–49. Santa Fe, New Mexico, USA: Association for Computational Linguistics.</p>
</div>
<div id="ref-bengio1993">
<p>Bengio, Y., P. Frasconi, and P. Simard. 1993. “The Problem of Learning Long-Term Dependencies in Recurrent Networks.” In <em>IEEE International Conference on Neural Networks</em>, 1183–8 vol.3. <a href="https://doi.org/10.1109/ICNN.1993.298725">https://doi.org/10.1109/ICNN.1993.298725</a>.</p>
</div>
<div id="ref-devlin2019">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div id="ref-devries2019">
<p>de Vries, Wietse, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. “BERTje: A Dutch BERT Model.” <em>arXiv Preprint arXiv:1912.09582</em>.</p>
</div>
<div id="ref-esuli2010">
<p>Esuli, Andrea, and Fabrizio Sebastiani. 2010. “Evaluating Information Extraction.” In <em>Multilingual and Multimodal Information Access Evaluation</em>, edited by Maristella Agosti, Nicola Ferro, Carol Peters, Maarten de Rijke, and Alan Smeaton, 100–111. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/978-3-642-15998-5_12">https://doi.org/10.1007/978-3-642-15998-5_12</a>.</p>
</div>
<div id="ref-goodfellow2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-graves2014">
<p>Graves, Alex. 2014. “Generating Sequences with Recurrent Neural Networks.” <em>arXiv:1308.0850 [Cs]</em>, June. <a href="http://arxiv.org/abs/1308.0850">http://arxiv.org/abs/1308.0850</a>.</p>
</div>
<div id="ref-huang2015">
<p>Huang, Zhiheng, Wei Xu, and Kai Yu. 2015. “Bidirectional LSTM-CRF Models for Sequence Tagging.” <em>arXiv Preprint arXiv:1508.01991</em>.</p>
</div>
<div id="ref-lafferty2001">
<p>Lafferty, John, Andrew Mccallum, and Fernando Pereira. 2001. <em>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</em>.</p>
</div>
<div id="ref-lample2016">
<p>Lample, Guillaume, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. “Neural Architectures for Named Entity Recognition.” In <em>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 260–70. San Diego, California: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N16-1030">https://doi.org/10.18653/v1/N16-1030</a>.</p>
</div>
<div id="ref-ma2016">
<p>Ma, Xuezhe, and Eduard Hovy. 2016. “End-to-End Sequence Labeling via Bi-Directional LSTM-CNNs-CRF.” <em>arXiv:1603.01354 [Cs, Stat]</em>, May. <a href="http://arxiv.org/abs/1603.01354">http://arxiv.org/abs/1603.01354</a>.</p>
</div>
<div id="ref-manning2019a">
<p>Manning, Christopher. 2019a. <em>CS224n: Natural Language Processing with Deep Learning</em>. Stanford NLP.</p>
</div>
<div id="ref-manning2019">
<p>Manning, Christopher. 2019b. “Stanford CS 224N | Natural Language Processing with Deep Learning.” http://web.stanford.edu/class/cs224n/.</p>
</div>
<div id="ref-martin2019">
<p>Martin, Louis, Benjamin Muller, Pedro Javier Ortiz Su’arez, Yoann Dupont, Laurent Romary, ’Eric Villemonte de la Clergerie, Djam’e Seddah, and Benoît Sagot. 2019. “CamemBERT: A Tasty French Language Model.” <em>arXiv Preprint arXiv:1911.03894</em>.</p>
</div>
<div id="ref-mikolov2013">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv:1301.3781 [Cs]</em>, September. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>
</div>
<div id="ref-nothman2013">
<p>Nothman, Joel, Nicky Ringland, Will Radford, Tara Murphy, and James R. Curran. 2013. “Learning Multilingual Named Entity Recognition from Wikipedia.” <em>Artificial Intelligence</em>, Artificial Intelligence, Wikipedia and Semi-Structured Resources, 194 (January): 151–75. <a href="https://doi.org/10.1016/j.artint.2012.03.006">https://doi.org/10.1016/j.artint.2012.03.006</a>.</p>
</div>
<div id="ref-peters2017">
<p>Peters, Matthew E., Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. “Semi-Supervised Sequence Tagging with Bidirectional Language Models.” <em>arXiv Preprint arXiv:1705.00108</em>.</p>
</div>
<div id="ref-peters2018">
<p>Peters, Matthew, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. “Deep Contextualized Word Representations.” In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, 2227–37. New Orleans, Louisiana: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N18-1202">https://doi.org/10.18653/v1/N18-1202</a>.</p>
</div>
<div id="ref-reimers2017">
<p>Reimers, Nils, and Iryna Gurevych. 2017. “Reporting Score Distributions Makes a Difference: Performance Study of LSTM-Networks for Sequence Tagging.” <em>arXiv:1707.09861 [Cs, Stat]</em>, July. <a href="http://arxiv.org/abs/1707.09861">http://arxiv.org/abs/1707.09861</a>.</p>
</div>
<div id="ref-sutskever2014">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3104–12.</p>
</div>
<div id="ref-tjongkimsang2002">
<p>Tjong Kim Sang, Erik F. 2002. “Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.” In <em>COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002)</em>.</p>
</div>
<div id="ref-tjongkimsang2003">
<p>Tjong Kim Sang, Erik F., and Fien De Meulder. 2003. “Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.” In <em>Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</em>, 142–47.</p>
</div>
<div id="ref-vaswani2017">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems 30</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 5998–6008. Curran Associates, Inc.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p><a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/ONE_HOT_EMBEDDINGS.md">https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/ONE_HOT_EMBEDDINGS.md</a><a href="methods.html#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p><a href="https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/CHARACTER_EMBEDDINGS.md">https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/CHARACTER_EMBEDDINGS.md</a><a href="methods.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>More information on these models can be found in the <a href="https://github.com/flairNLP/flair/blob/083eb114505ede0ea3070317a5a08d13fc71891b/resources/docs/embeddings/FLAIR_EMBEDDINGS.md">documentation of flairNLP on GitHub</a><a href="methods.html#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p><a href="http://jalammar.github.io/illustrated-transformer/">jalammar.github.io/illustrated-transformer/</a><a href="methods.html#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p><a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD benchmark leaderbord</a><a href="methods.html#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p><a href="https://github.com/huggingface/transformers">GitHub: huggingface/transformers</a><a href="methods.html#fnref10" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="researchproblem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="results.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/arthur-arthur/MASTAT_thesis/edit/master/03-methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Thesis_Arthur_Leloup.pdf", "Thesis_Arthur_Leloup.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
